{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfba48294b1c4da479e3657c5650058",
     "grade": false,
     "grade_id": "cell-e61ca27a079c37a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting started with neural networks\n",
    "\n",
    "For this assignment, we're going to take a first look at neural networks. Neural networks are a model constructed of a large combination of artificial neurons. We can use logistic regression as the building block for each of these artificial neurons and create a whole network of logistic regression units.\n",
    "\n",
    "Logistic regression is not the only building block we could use, there are a lot of different types of units and connections used in neural networks. However, logistic regression is the block that was historically used in the first neural networks, and in many ways, it is the most intuitive regression unit to create such a network with. So, for this assignment, we'll build some neural networks using only logistic regression units. In the language of neural networks, we're building a fully connected neural network with sigmoid activations at every layer.\n",
    "\n",
    "This notion of combining several elemental building blocks or *modules*, in\n",
    "order to create a more complex network, is at the heart of neural networks. In\n",
    "fact, many of the recent advances in AI come from *deep* neural networks, which\n",
    "means they are the result of combining **a lot** of these modules. So, building\n",
    "the modules in such a way that we can easily stack many of them, will be\n",
    "essential.\n",
    "\n",
    "This assignment consists of three parts. In the first part you will learn more about modularity. You will re-implement logistic regression, but now in such a way that it can be used as a module in a neural network. In the second part you will use this logistic regression module to build a neural network. In the last part you will have a look at how you could implement the training of a single logistic regression module. This last part sets you up for the next module in which you will learn how to deal with the training of a complete neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d84206b83d6950d6abb219ccb87151c",
     "grade": false,
     "grade_id": "cell-b1f20cd20be6c6a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Modularity (Logistic Regression)\n",
    "\n",
    "The first module we will build, as mentioned, is logistic\n",
    "regression. You've already programmed this algorithm of course, but we'll\n",
    "change the implementation to make it better fit with the idea of modularity.\n",
    "None of these changes will modify the core steps of Logistic\n",
    "regression, but they will change the way we program and mathematically\n",
    "represent the algorithm. The 3 main changes will be:\n",
    "\n",
    "1. Changing the way the bias term $\\theta_0$ is added to the inputs. Up to now\n",
    "we've always added a column of 1's to the inputs, so the bias parameter\n",
    "could just be handled the same as every other parameter. However, if we stack a\n",
    "lot of modules together, that solution would become very messy.\n",
    "\n",
    "2. Viewing every step of the algorithm as part of a computational graph, even\n",
    "a simple step like adding the bias. This might seem like a strange step at\n",
    "first, but it will make trying to learn the parameters of all these modules a\n",
    "lot easier.\n",
    "\n",
    "3. Implementing the algorithm using *Object Oriented* programming, making it\n",
    "possible to create several *instances* of the algorithm, which can then easily\n",
    "be stacked and linked together, creating a large network of these smaller\n",
    "self-contained modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3893ba7654f0654565d2dc9108ea0d7e",
     "grade": false,
     "grade_id": "cell-a3362b9b42af4abc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 1: the bias term \n",
    "\n",
    "#### The classical approach\n",
    "\n",
    "The classical approach to add a bias term in logistic regression is by adding a $1$ to the input $x$ and adding a term $\\theta_0$ to the parameter vector $\\theta$:\n",
    "\n",
    "$$\n",
    "\\tilde{x} =\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_2 & \\cdots & x_n\n",
    "\\end{pmatrix}, \n",
    "\\theta =\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 & \\theta_1 & \\theta_2 & \\cdots & \\theta_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So to compute the output (hypothesis) $\\hat{y}$ you just have to multiply the $\\tilde{x}$ and $\\theta$ vectors before applying the sigmoid function $g$:\n",
    "\n",
    "$$\n",
    "z = \\tilde{x} \\cdot \\theta \\\\\n",
    "\\hat{y} = g(z) \\\\\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Working out these equations gives the following result:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g(\\theta_0 + \\theta_1 \\cdot x_ 1 + \\theta_2 \\cdot x_1 + \\cdots  + \\theta_n \\cdot x_n)\n",
    "$$\n",
    "\n",
    "Where you can see that $\\theta_0$ is the bias value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f94c5f2ba66493bd70bfd2d34c3b6674",
     "grade": false,
     "grade_id": "cell-02d1958d987729c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Add the bias explicitly\n",
    "\n",
    "This approach becomes messy when we take logistic regression to be a module that can be chained to other modules, like we would for neural networks. So instead we're going to explicitly add the bias term:\n",
    "\n",
    "$$\n",
    "z = x \\cdot \\theta + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "When you work out these equations you can see that it is mathematically identical to the previous result, with the only difference that the bias term $\\theta_0$ is now called $b$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g(\\theta_1 \\cdot x_ 1 + \\theta_2 \\cdot x_1 + \\cdots  + \\theta_n \\cdot x_n + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81272b6b0e654b00286a34641908cbe4",
     "grade": false,
     "grade_id": "cell-d27a4e34c357a7bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Vectorize with $\\oplus$\n",
    "\n",
    "Although the new representation is mathematically the same as the classic representation, you have to pay attention when vectorizing this. When we want to calculate the output for multiple samples at once, so $X$ is an $m \\times n$ matrix ($n$ features, $m$ samples), you would get the equations:\n",
    "\n",
    "$$\n",
    "z = X \\cdot \\theta \\color{red} + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "Here $X \\cdot \\theta$ is a vector containing $m$ values, but $b$ is a simple scalar. In linear algebra those *cannot be added*. However numpy has a way around this. It uses a system called broadcasting to apply the addition of $b$ to each of the elements of $X \\cdot \\theta$. We will use the symbol $\\oplus$ to denote numpy style addition. Strictly speaking, this operation is not a part of linear algebra and so you will not find it in any literature, but it is very convenient to use in this context. So, the vectorized equations can now be written as: \n",
    "\n",
    "$$\n",
    "z = X \\cdot \\theta \\oplus b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aca65a8e77bef720eea70a2f84dac97f",
     "grade": false,
     "grade_id": "cell-98d275dae6e51b8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradient terms\n",
    "\n",
    "This also changes the gradient. Or we should say gradients. Next to computing the gradient for $\\theta$ you have to also compute the gradient for $b$. The gradient for $\\theta$ is the same as for the classic approach, but the one for $b$ differs slightly:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)x_j^i \\\\\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)\n",
    "$$\n",
    "\n",
    "The derivation of $\\frac{\\partial J_{\\theta,b}}{\\partial b}$ is a bit long for this notebook. But the result shouldn't be too surprising; after all, the bias term in the original solution was computed by setting $x_0$ to $1$, and when you substitute $x_j$ by $1$ in the first equation above, you get the second.\n",
    "\n",
    "When represented as matrix operations, this comes down to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial \\theta} = \\frac{1}{m}X^T(\\hat{y} -y) \\\\\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)\n",
    "$$\n",
    "\n",
    "> Note that the gradient of $b$ isn't reduced any further into linear algebra terms. It is possible to do so, but it isn't very helpful for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47db5ab69f653cfb9b3d0dacccf58d2f",
     "grade": false,
     "grade_id": "cell-2435918f042a4ab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 2: computational graphs\n",
    "\n",
    "\n",
    "#### Computational graphs in general\n",
    "\n",
    "A convenient way to represent the mathematics in neural networks is using computational graphs. It allows you to represent a neural network as data flowing through a network of computation. Especially when we're going to look at backpropagation (i.e. learning), this is particularly useful. We will already start using computational graphs here, to get used to the notation.\n",
    "\n",
    "The following graph represents the computation $c= a + b$\n",
    "\n",
    "<img src=\"src/cg1.svg\" width=\"20%\">\n",
    "\n",
    "The computation $+$ is represented as a node where the data from variables $a$ and $b$ flow into.\n",
    "\n",
    "So if the values of $a$ and $b$ are $2$ and $1$, respectively the data-flow through the computational graph would look like this:\n",
    "\n",
    "<img src=\"src/cg2.svg\" width=\"20%\">\n",
    "\n",
    "Let's have a look at a second, more complex example, $c = ln(ab + 2a^2)$:\n",
    "\n",
    "<img src=\"src/cg3.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f9421a105748e75066a2a2a98705903",
     "grade": false,
     "grade_id": "cell-bd91c429eb6aeae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Logistic regression as computational graph\n",
    "\n",
    "Similarly we can represent logistic regression as a computational graph. We adopt the convention to represent the non vectorized version of the equations for the computational graph. So we will assume a single input vector $x$ and a single output value $y$.\n",
    "\n",
    "The computational graph for logistic regression looks like this:\n",
    "\n",
    "<img src=\"src/Logistic Regression.svg\" width=\"40%\">\n",
    "\n",
    "And we represent the gradients by dashed arrows going in the opposite direction:\n",
    "\n",
    "<img src=\"src/Logistic Regression gradients.svg\" width=\"40%\">\n",
    "\n",
    "With the relatively low complexity of logistic regression, this representation might not feel very useful yet. But it will start to make more sense once we move on to neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc4f1204fd3104b087a15163430a5763",
     "grade": false,
     "grade_id": "cell-0f994012c73af2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 3: implemented as a Python class\n",
    "\n",
    "#### Gradient descent algorithms\n",
    "\n",
    "Before we implement logistic regression as a class, it is good to think about its specifications. All gradient descent algorithms (e.g., linear and logistic regression) follow the same pattern: \n",
    "\n",
    "- They all predict some output $y$ based on some input $X$ (the _forward_ pass).\n",
    "- There is some cost function (loss) that allows us to quantify how good our current predictions are.\n",
    "- They all compute some gradient that tells us how we should change the parameters of the model to give better predictions next time (the _backward_ pass).\n",
    "- There is some form of gradient descent in which we take a step in the opposite direction of the gradient.\n",
    "\n",
    "#### Using classes\n",
    "\n",
    "In the previous modules you implemented these steps for both linear and logistic regression using separate variables and functions. This made running the algorithm and managing the data quite messy. The idea of using a class is that we can bundle all of this together and make running the descent algorithm much cleaner. \n",
    "\n",
    "Before we implement the class, let's have a look at how it is *intended to be used*. What we would like is that (once the class is defined) we can create a new logistic regression model and set the initial values of the learning parameters ($\\Theta$ and $b$) by simply creating a new instance. For example, if we want to have a logistic model with 4 inputs:\n",
    "\n",
    "    my_model = Logistic(4)\n",
    "\n",
    "Then, if we want do a step of the gradient descent algorithm for some input `X` and some given output `y`, we want to be able to do something like this:\n",
    "\n",
    "    # predict the output for X\n",
    "    my_model.forward(X)\n",
    "    \n",
    "    # compute the gradients\n",
    "    my_model.backward(y)\n",
    "    \n",
    "    # update the learning parameter theta and b, using a learning rate alpha\n",
    "    my_model.step(alpha)\n",
    "\n",
    "Note that nowhere above you see any variables representing the learning parameters or the gradients. This is the crucial part: they are encapsulated inside the class. So once the class is defined we don't have to think about them anymore. Those details will be abstracted away by our design. \n",
    "\n",
    "The function `optimize()` is the function that will run the gradient descent given the model that you will implement. The function `optimize()` is already implemented a little bit further down. Have a look at it, you will see it implements the logistic algorithm exactly as described here. However, it will not work yet, because the implementation of the class `Logistic` is not finished yet. This is left up to you to finish.\n",
    "\n",
    "#### Class specification\n",
    "\n",
    "As we established, all gradient descent algorithms are very similar and so the classes implementing them will all look very similar. We can rely on the previously mentioned commonalities to define a general template for any of those algorithms. We will agree that any class that represents a gradient descent module, will implement the following methods:\n",
    "\n",
    "- `__init__`, set initial learning parameter.\n",
    "- `forward`, given some input $x$ predict an output $\\hat{y}$.\n",
    "- `backward`, compute all gradients of the parameters (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "- `step`, update all parameters (based on the gradients computed in `backward`, and a learning rate `alpha`).\n",
    "- `cost`, compute the cost (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "\n",
    "By observing this template we should be able to apply the gradient descent algorithm to any module by repeatedly calling the methods `forward`, `backward` and `step` in succession, until we reach convergence (whatever our criterion for convergence might be).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68f08130ba871b29afcf93be5c2a3d75",
     "grade": false,
     "grade_id": "cell-f5a3c25f51774e0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1\n",
    "\n",
    "Now we're going to re-implement logistic regression as a class. We've made a start with the implementation, but the class `Logistic` below is not completely finished. Your task is to complete the `TODO`'s. Of course, you can re-use some code from the logistic regression assignment from last week, but bear in mind that there are some changes in implementation regarding the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bcd3610cab7d3480097830439c444c2",
     "grade": true,
     "grade_id": "cell-949db3489a20fe97",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self, s_in):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "        \n",
    "        # the parameters (updated in step)\n",
    "        self.theta = np.zeros(s_in)\n",
    "        self.b = 0\n",
    "        \n",
    "        # the input and output values (set in forward)\n",
    "        self.X = None\n",
    "        self.y_hat = None\n",
    "        \n",
    "        # the gradient values (set in backward)\n",
    "        self.d_theta = None\n",
    "        self.d_b = None\n",
    "  \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.theta, self.b and x.\"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        # z values\n",
    "        self.z = np.matmul(self.X, self.theta) + self.b\n",
    "        \n",
    "        # logistic function\n",
    "        self.y_hat = 1 / (1 + np.exp(-self.z))\n",
    "        \n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, y):\n",
    "        \"\"\" Compute gradients self.d_theta and self.d_b, based on self.y_hat and y.\"\"\"\n",
    "        \n",
    "        self.y = y\n",
    "        \n",
    "        # set m to length of x\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        # calculate d_theta\n",
    "        fraction = (1/m)\n",
    "        error = self.y_hat - y\n",
    "        sum_error = np.matmul(self.X.T, error)\n",
    "        self.d_theta = fraction * sum_error\n",
    "        \n",
    "        # calculate d_b\n",
    "        fraction = (1/m)\n",
    "        sum_error = np.sum(self.y_hat - y)\n",
    "        self.d_b = fraction * sum_error\n",
    "\n",
    "        return self.d_theta, self.d_b\n",
    "\n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" Update self.theta and self.b based on self.d_theta, self.d_b, and alpha.\"\"\"\n",
    "\n",
    "        self.theta = self.theta - alpha * self.d_theta\n",
    "        self.b = self.b - alpha * self.d_b\n",
    "        \n",
    "        return self.theta, self.b\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cost(self, y = None):\n",
    "        \"\"\" Compute cost, based on prediction, self.y_hat, and target: y (or self.y).\"\"\"\n",
    "        if not y:\n",
    "            y = self.y\n",
    "            \n",
    "        # return cost based on:\n",
    "        # - the predicted output (self.y_hat) \n",
    "        # - and the actual values (y or self.y)\n",
    "        \n",
    "        # set m to length of x\n",
    "        m = self.X.shape[0]\n",
    "    \n",
    "        # cost = -1 / m * np.sum(y * np.log(X) + (1 - y) * (np.log(1 - X)))\n",
    "        part1 = np.multiply(y, np.log(self.y_hat))\n",
    "        part2 = np.multiply((1 - self.y), np.log(1 - self.y_hat))\n",
    "        summ = np.sum(part1 + part2)\n",
    "        fraction = - (1 / m)\n",
    "        cost = (fraction * summ)\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfd212942c1b50cff31843020a44a82",
     "grade": false,
     "grade_id": "cell-0988c34a857ee1e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The function `optimize` below runs the gradient descent algorithm. If you implemented the methods above correctly, this function should work for the `Logistic` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0374be0d309b185925d1cfea1d301660",
     "grade": false,
     "grade_id": "cell-478f0a7ed43cc59c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(model, X, y, alpha, iterations = 500):\n",
    "    \"\"\"Apply gradient descent for `iterations` number of steps to any learning model that\n",
    "       implements the correct functions, i.e. forward(), backward(), step() and cost().\"\"\"\n",
    "    costs = []\n",
    "    for i in tqdm(range(iterations)):\n",
    "        # descent\n",
    "\n",
    "        model.forward(X)\n",
    "        model.backward(y)\n",
    "        model.step(alpha)\n",
    "        \n",
    "        # keep track of costs\n",
    "        costs.append(model.cost())\n",
    "        \n",
    "        # check for divergence (alpha too big)\n",
    "        if len(costs) >= 2 and (costs[-2] - costs[-1]) < 0:\n",
    "            print(f'Diverging at iteration {len(costs)}')\n",
    "            return costs  \n",
    "    return costs\n",
    "\n",
    "def confusion_matrix(p, y):\n",
    "    return np.vstack((p, 1 - p)) @ np.vstack((y, 1 - y)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "528897cbab28740a5a941efab4294127",
     "grade": false,
     "grade_id": "cell-e431931347410616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test your solution\n",
    "Let's start by loading the titanic data again. This time we start with a version of this dataset that we've already cleaned up and is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4623ca1bdfba189f5ba6d2945568798",
     "grade": false,
     "grade_id": "cell-be1b144451e8688f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def round_output(x):\n",
    "    return (x >= 0.5) * 1\n",
    "\n",
    "data = pd.read_csv('data/clean_titanic.csv', index_col = 0)\n",
    "y = data['Survived']\n",
    "X = data.drop('Survived', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3,  random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f65529d5c9e5760a1e3acab7e71e21c2",
     "grade": false,
     "grade_id": "cell-c4b3154a46240e5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below runs the gradient descent and shows the confusion matrix.\n",
    "\n",
    "If all went well, the confusion matrix should look the same to the one you found in the previous assignment for logistic regression. The values should be close to this:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "& y=1 & y=0 \\\\\n",
    "\\hat{y}=1 & 69 & 21 \\\\\n",
    "\\hat{y}=0 & 23 & 155\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b91e30b71d02dbc66a616c051df8dfc",
     "grade": false,
     "grade_id": "cell-185b233c22dac50a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 307.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 69  21]\n",
      " [ 23 155]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train model and predict\n",
    "logistic_model = Logistic(X_train.shape[1])\n",
    "optimize(logistic_model, X_train, y_train, 0.2)\n",
    "predictions = round_output(logistic_model.forward(X_test))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57021ebb293d743fa6c077230df91b21",
     "grade": false,
     "grade_id": "cell-67e1d9dbff5fec1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Neural networks\n",
    "\n",
    "### Logistic layer\n",
    "\n",
    "As you have learned from Andrew's videos, classic neural networks consist of multiple logistic layers. Each layer is essentially Logistic Regression, but instead of one single output, a logistic layer can have multiple outputs. And those outputs serve as the inputs for the next layer.\n",
    "\n",
    "#### Intuitive solution\n",
    "\n",
    "So, let's say we want to create a logistic layer that outputs $o$ values labeled $y_1, y_2, \\ldots y_o$. One way you could theoretically accomplish this is by just creating $o$ different logistic regression units with $o$ individual $\\theta$ parameters, and $o$ individual $b$ parameters, that all operate on the input $x$, like so:\n",
    "\n",
    "<img src=\"src/Mulit Logistic Regression.svg\" width=\"50%\">\n",
    "\n",
    "Where we could create the output by composing all the outputs of the individual units into one vector: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{pmatrix} y_1 & y_2 & \\cdots & y_o \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Efficient solution\n",
    "\n",
    "This makes intuitive sense but it is not particularly elegant, and it will not be very efficient if you would implement it this way. Generally we always try to express everything as much as possible as a single matrix operation. This makes use of the hardware you use for the computation much more efficiently. \n",
    "\n",
    "We can rewrite the above graph by creating a single weight matrix combining all the weights $\\theta_i$ and a vector $b$ containing all the biases $b_i$:\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{pmatrix} \n",
    "-\\theta_1 - \\\\ \n",
    "-\\theta_2 - \\\\ \n",
    "\\vdots \\\\\n",
    "-\\theta_o -  \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix} \n",
    "b_1 \\\\ \n",
    "b_2 \\\\ \n",
    "\\vdots \\\\\n",
    "b_o\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The resulting computational graph:\n",
    "\n",
    "<img src=\"src/cg4.svg\" width=\"40%\">\n",
    "\n",
    "In the form of equations:\n",
    "\n",
    "$$\n",
    "z = W \\cdot x + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "Which are essentially the same equations as for logistic regression. But, instead of the vector $\\theta$, we have an $o\\times n$ weight matrix called $W$ (for an input containing $n$ features and $o$ output values). And the bias $b$ is not a scalar value, but a vector of size $o$.\n",
    "\n",
    "> You've seen two possible solutions: The composed logistic regression units and the representation as a single mathematical system. Verify for yourself that they are mathematically equivalent!\n",
    "\n",
    "> Note that Andrew uses slightly different conventions here: He uses the symbol $\\Theta$ for the weight matrix. And he doesn't use a separate $b$ vector, but instead relies on the trick of augmenting the input vector with $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4874e7e168afde29acae0bb185a93e26",
     "grade": false,
     "grade_id": "cell-eb115918f19a19bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic layer vectorized\n",
    "\n",
    "We have to pay attention when we want to compute multiple samples at once. When $X$ is an $m \\times n$ matrix ($n$ features, $m$ samples) and we want to compute $o$ different outputs, we'll have to compute the matrix $\\hat{Y}$, which then will be of dimension $m \\times o$, all at once:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = g(Z)\n",
    "$$\n",
    "\n",
    "Here $Z$ will be the input of the Logistic function, so this must be a matrix of those same dimensions $m \\times o$ ($o$ outputs, $m$ samples). To compute $Z$ we'll need $b$, the vector containing $o$ bias terms and $W$, which is an $o \\times n$ matrix of weights. The main difference with simple Logistic Regression is that you need to transpose the weight matrix in order to keep the dimensions correct for $Z$ in the final output:\n",
    "\n",
    "$$\n",
    "Z = X \\cdot W^{T} \\oplus b\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "So if we have a logistic layer with 2 inputs and 3 outputs, with the following equations:\n",
    "\n",
    "$$\n",
    "Z = X \\cdot W^{T} \\oplus b\\\\\n",
    "\\hat{Y} = g(Z)\n",
    "$$\n",
    "\n",
    "The values are represented by the following matrices (for an input containing $m$ samples):\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} \\\\ \n",
    "x_{2,1} & x_{2,2} \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m,1} & x_{m,2} \\\\ \n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "\\hat{y}_{1,1} & \\hat{y}_{1,2} & \\hat{y}_{1,3} \\\\ \n",
    "\\hat{y}_{2,1} & \\hat{y}_{2,2} & \\hat{y}_{2,3} \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "\\hat{y}_{m,1} & \\hat{y}_{m,2} & \\hat{y}_{m,3}\n",
    "\\end{pmatrix}\n",
    ",\n",
    "W = \n",
    "\\begin{pmatrix}\n",
    "W_{1,1} & W_{1,2} \\\\ \n",
    "W_{2,1} & W_{2,2} \\\\ \n",
    "W_{3,1} & W_{3,2} \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix}\n",
    "b_{1} & b_{2} & b_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If you enter those values into the equations, you will get the following output:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "g(W_{1,1}x_{1,1} + W_{1,2}x_{1,2} + b_1) & g(W_{2,1}x_{1,1} + W_{2,2}x_{1,2} + b_2) & g(W_{3,1}x_{1,1} + W_{3,2}x_{1,2} + b_3)\\\\ \n",
    "g(W_{1,1}x_{2,1} + W_{1,2}x_{2,2} + b_1) & g(W_{2,1}x_{2,1} + W_{2,2}x_{2,2} + b_2) & g(W_{3,1}x_{2,1} + W_{3,2}x_{2,2} + b_3) \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "g(W_{1,1}x_{3,1} + W_{1,2}x_{3,2} + b_1) & g(W_{2,1}x_{3,1} + W_{2,2}x_{3,2} + b_2) & g(W_{3,1}x_{3,1} + W_{3,2}x_{3,2} + b_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Verify this for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6f1bbc37209e6635ae3bc3b58ca3909",
     "grade": false,
     "grade_id": "cell-b8d7949f80c6babb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "Implement the class `LogisticLayer` below. You will only have to implement the `init` and the `forward` method. Note that the `forward` method should be very similar to the one of logistic regression. Following the mathematics outlined above, the forward method should implement the following computational graph:\n",
    "\n",
    "<img src=\"src/cg5.svg\" width=\"40%\">\n",
    "\n",
    "The `init` method should set the $W$ and $b$ parameters (`self.W` and `self.b`) as `np.array`'s of the right dimensions, as specified by `s_in` (the input size) and `s_out` (the output size). Both $W$ and $b$ should start out filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d94a8a85e609f40ff879b4e466bf2ea2",
     "grade": true,
     "grade_id": "cell-f502a850337d44e3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "\n",
    "        # set W to wshape output * input\n",
    "        self.W = np.zeros((s_out, s_in))\n",
    "\n",
    "        # set b to number of features\n",
    "        self.b = np.zeros(s_out)\n",
    " \n",
    "        \n",
    "    def manually_set_weights(self, W, b):\n",
    "        \"\"\" Set weights manually. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert self.W.shape == W.shape, \"W: wrong shape\"\n",
    "        assert self.b.shape == b.shape, \"b: wrong shape\"\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.theta, self.b and X.\"\"\"\n",
    "       \n",
    "        self.X = X\n",
    "        \n",
    "        # z values\n",
    "        self.z = np.matmul(self.X, self.W.T) + self.b\n",
    "        \n",
    "        # logistic function\n",
    "        self.y_hat = 1 / (1 + np.exp(-self.z))\n",
    "        \n",
    "        return self.y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bafb68cf2a0e5ed593bd3c602a33e64",
     "grade": false,
     "grade_id": "cell-728a23770532ff4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb7453fbd1c9f215953984d762904e20",
     "grade": true,
     "grade_id": "cell-692b8ef24379540b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# test Logistic Layer\n",
    "testLL = LogisticLayer(3,2)\n",
    "\n",
    "# some arbitrary test values\n",
    "testX = np.array([[-1,  0,  1]])\n",
    "testY = np.array([[0.26894142, 0.73105858]])\n",
    "\n",
    "# set weights\n",
    "testLL.manually_set_weights(np.array([[1, 1, 1], [0, 0, 0]]), np.array([-1, 1]))\n",
    "\n",
    "np.testing.assert_allclose(testLL.forward(testX), testY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbb179094ea8d06e0cfb8cb22d4c59f3",
     "grade": false,
     "grade_id": "cell-86c44b2427810801",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representing logic functions\n",
    "\n",
    "The function above can now produce a network output for any $o$ number of neurons, using a single matrix multiplication, with just those slightly modified Logistic regression functions from before. The only thing needed to produce $o$ outputs is a weight matrix $W$ and bias vector $b$ of the correct dimensions, and then the result should be a hypothesis matrix $\\hat{Y}$ with dimensions $m \\times o$.\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "In the theory videos, Andrew describes how to encode the boolean *AND* function in neural network weights. This is of course a not very realistic toy example. You wouldn't use neural networks for logic operations, but it can be interesting to look at this example to get a feel for how neural networks work. \n",
    "\n",
    "Recreate Andrew's example using the `LogisticLayer` class to build a really simple network called `andLL`, with 2 inputs and 1 output (i.e. just basic logistic regression). Andrew's example assumes that the bias is part of the $\\Theta$ matrix, whereas we split that up into a separate weight matrix ($W$) and bias vector ($b$). So your solution will look a bit different from the one Andrew proposes.\n",
    "\n",
    "- Make sure $W$ is a matrix of the correct shape and has the correct values for each of the required $W_{ji}$.\n",
    "- Make sure the $b$ is a vector of the correct size.\n",
    "- The network output should be a *column* vector of 4 outputs, corresponding to the logical *AND* of each of the 4 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a2d6935448bd9c6e7bb8f3cab846439",
     "grade": true,
     "grade_id": "cell-f814f9b203f7773a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "'''\n",
    "These numbers are chosen to get teh right output. \n",
    "For the output, it is important that the weights are some positive number, less than the negative number of the bias. \n",
    "'''\n",
    "\n",
    "# Implement the class LogisticLayer\n",
    "andLL = LogisticLayer(2, 1)\n",
    "\n",
    "# set weights to 10 and bias to 15\n",
    "andLL.manually_set_weights(np.array([[10, 10]]), np.array([-15]))\n",
    "\n",
    "print(round_output(andLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42283bde7365a903c1b0dd22fd033a02",
     "grade": false,
     "grade_id": "cell-dae0683aeb5cbc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1693f98b60661c65680ee9c7210a0a28",
     "grade": true,
     "grade_id": "cell-4b8a17ad79da5bf6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(round_output(andLL.forward(X)), andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5a7b46e3ab8ed54a84c161f30bc5331",
     "grade": false,
     "grade_id": "cell-d69d93a1e586d8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4\n",
    "\n",
    "In the same spirit we can encode some other logical operations. Encode the logical operations *OR* and *NAND* below with neural networks `orLL` and `nandLL` respectively. If you're not sure of their definitions, the expected output is provided.\n",
    "\n",
    "Hint: Think how the input corresponds to the output and work your way back. For which output do you need $z$ to be (very) high, and for which output do you need $z$ to be very low. How does this correspond to the input? How can you manipulate $W$ and $b$ to achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "814ba2c03b44480327c8f8c59650f149",
     "grade": true,
     "grade_id": "cell-f54b68efea82e510",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "orY = np.array([[0], [1], [1], [1]])\n",
    "nandY = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "'''\n",
    "This part of the programme encodes the logical operations for OR and NAND with orLL and nandLL respectively\n",
    "These numbers are chosen to get teh right output. \n",
    "'''\n",
    "\n",
    "# Implement the class LogisticLayer\n",
    "orLL = LogisticLayer(2, 1)\n",
    "\n",
    "# Set weights to 10 and bias to 15. Discussed by Andrew Ng (videos) so I used his weights and the bias.\n",
    "orLL.manually_set_weights(np.array([[20, 20]]), np.array([-10]))\n",
    "\n",
    "print(round_output(orLL.forward(X)))\n",
    "\n",
    "# Implement the class LogisticLayer\n",
    "nandLL = LogisticLayer(2, 1)\n",
    "\n",
    "# Set weights to 10 and bias to 15 - important to pick some negative weights which are smaller that the bias.\n",
    "nandLL.manually_set_weights(np.array([[-10, -10]]), np.array([15]))\n",
    "\n",
    "print(round_output(nandLL.forward(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd3ab1b9dc5874910718fecf1c6fa5e",
     "grade": true,
     "grade_id": "cell-8a88ac25456bba7c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(round_output(orLL.forward(X)), orY)\n",
    "np.testing.assert_array_equal(round_output(nandLL.forward(X)), nandY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6357f27b65a5dcef736d2a18632d75af",
     "grade": false,
     "grade_id": "cell-701963f9a602cb60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5\n",
    "\n",
    "Of course, a logistic layer can have more than one output. Create a new logistic layer called `nand_orLL` that has two outputs, corresponding the *NAND* and *OR* values for every input respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d745d9ed664e8e9477740e1b99f450e7",
     "grade": true,
     "grade_id": "cell-766985420c457b0c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "nand_orY = np.array([[0, 1], [1, 1], [1, 1], [1, 0]])\n",
    "\n",
    "# Implement the class LogisticLayer\n",
    "nand_orLL = LogisticLayer(2, 2)\n",
    "\n",
    "# set weights to 10 and bias to 15. Discussed by Andrew Ng (videos) so his used those weights and the bias.\n",
    "nand_orLL.manually_set_weights(np.array([[20, 20], [-15, -15]]), np.array([-10, 15]))\n",
    "\n",
    "print(round_output(nand_orLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0f06eeddf4d750d87d6ab69a02cb845",
     "grade": false,
     "grade_id": "cell-f570f3425a0a6481",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2356d0b9cfbe1015bcd1f666764cf196",
     "grade": true,
     "grade_id": "cell-7db7ad00b8aa8e6b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(round_output(nand_orLL.forward(X)), nand_orY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e4defa96e33591ef778bb33e6ec9a5",
     "grade": false,
     "grade_id": "cell-0fedcb18d1a1d204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6\n",
    "\n",
    "Last one: Try to make a network that models the exclusive or (XOR). If you can't get it to work, explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015f2ebc735136323e3eaa81318a3621",
     "grade": true,
     "grade_id": "cell-aaf6897d957ba8c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# See answer to question below\n",
    "\n",
    "# print(round_output(xorLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. If you can't get XOR to work, explain why you think this is below.**\n",
    "\n",
    "*XNOR would require a non-linear decision boundary so this cannot be done with our programme as is. You'd need another layer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eca8175fbf8b7b64278deb2247749e3f",
     "grade": false,
     "grade_id": "cell-dcc0550e49bea186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-layer neural networks\n",
    "\n",
    "For some problems, you need to add a *hidden* layer to the neural network. This is where the real expressive power of neural networks lies; combining layers of the network to represent *non-linear* features and solve complex, non-linear functions.\n",
    "\n",
    "A neural network with 2 layers can already learn *many* more functions than a single layer network. In general, you can stack $n$ different layers together and the basic principle does not change: The output matrix for one layer becomes the input matrix for the next layer, repeated for as many layers as there are in the network.\n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "The computational graph for a 2 layer network looks as follows:\n",
    "\n",
    "<img src=\"src/Multi layer NN.svg\" width=\"70%\">\n",
    "\n",
    "In general, the computational graph for a single layer looks like this:\n",
    "\n",
    "<img src=\"src/Single layer NN.svg\" width=\"35%\">\n",
    "\n",
    "#### The math\n",
    "\n",
    "Written as equations:\n",
    "$$\n",
    "Z_{i} = A_i \\cdot W_{i}^T \\oplus b \\\\\n",
    "A_{i+1} = g(Z_{i})\n",
    "$$\n",
    "\n",
    "Where the input of layer $i$ is $A_i$ and the output is $A_{i+1}$. For the first layer ($i = 0$) the input is our data $A_{0} = X$. For the last layer (in the example above $i = 1$) the output is the value we're predicting $A_{2} = \\hat{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d590bd2ea6fe57c7f29faf32105a9333",
     "grade": false,
     "grade_id": "cell-ef45de451451d89a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "We can now create a new class called `NN` containing a multilayer neural network. Every layer is a `LogisticLayer` object. The `NN` class follows the same template as `LogisticLayer` did, and so in principle this class should also be implementing all the relevant methods (`__init__`, `forward`, `backward`, `step`, and `cost`). However, for now we will limit ourselves to only the `__init__` and `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04603720247717ce17a322c70ba44107",
     "grade": false,
     "grade_id": "cell-60947c0262ef1fe8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 7\n",
    "\n",
    "The `NN` class is partially implemented. It contains an `init` method that, given a list with layer dimensions, will create a list with matching `LogisticLayer`objects.\n",
    "\n",
    "Your goal is to add a `forward` method to this class. It should not require you to write any new mathematical code. You should be able to rely on the `forward` of the logistic layers in `self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f9b012d830a25822b51def493061c65",
     "grade": true,
     "grade_id": "cell-ce857e0ba13db728",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layer_sizes = [2,2,1]):\n",
    "        \"\"\" Set initial layers. \"\"\"\n",
    "        self.layers = []\n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(LogisticLayer(s_in, s_out))\n",
    "            \n",
    "    def manually_set_weights(self, Ws, bs):\n",
    "        \"\"\" Provide list of weight matrices Ws and list of bias vectors bs. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert len(Ws)==len(self.layers), \"Ws: wrong length\"\n",
    "        assert len(bs)==len(self.layers), \"bs: wrong length\"\n",
    "        \n",
    "        for layer, W, b in zip(self.layers, Ws, bs):\n",
    "            layer.manually_set_weights(W, b)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" \n",
    "        Funtion compute prediction, y_hat, using forward function. It overwrites every self.y_hat.\n",
    "        To enable use in different layers. Function returns final prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        # set prediction to X to start\n",
    "        self.y_hat = X\n",
    "\n",
    "        # add a forward method to the class based on the logistic layers in self.layers, overwrite every self.y_hat\n",
    "        for layer in self.layers:\n",
    "            self.y_hat = layer.forward(self.y_hat)\n",
    "        \n",
    "        return self.y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7acac3cde5258b6f29c3b37c9574a0b0",
     "grade": false,
     "grade_id": "cell-6b339d15210e26be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d3ac9c672ae3019f4e48f782b2e1e03",
     "grade": true,
     "grade_id": "cell-91fff74d8978ae9f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "testNN = NN(layer_sizes = (3,2,2))\n",
    "\n",
    "# layer 0: 3(+1) -> 2\n",
    "testNN.manually_set_weights([np.array([[0.5, 1.0, 1.5], [-0.5, -1.0, -1.5]]), \n",
    "                             np.array([[1, 0], [0.5, 0.5]])],\n",
    "                            [np.array([-1, 1]),\n",
    "                             np.array([0, 1])])\n",
    "\n",
    "# output\n",
    "result = testNN.forward(np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]))\n",
    "answer = np.array([[0.56683301, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.62245933, 0.81757448],\n",
    "                   [0.69372123, 0.81757448],\n",
    "                   [0.59327981, 0.81757448],\n",
    "                   [0.67503753, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.70698737, 0.81757448]])\n",
    "\n",
    "np.testing.assert_allclose(result, answer)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9521320588cfc46014a70b74f0b85dd7",
     "grade": false,
     "grade_id": "cell-194a50cbd4b4cc36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 8\n",
    "\n",
    "Andrew Ng also discusses in detail how to encode the solution for the *XNOR* problem in neural network weights. The network should have 2 inputs, 2 hidden nodes and 1 output, and the necessary added bias nodes for the input and hidden layer.\n",
    "\n",
    "The same is true for the *XOR* network. Define a neural network called `xorNN` that produces the correct *XOR* output.\n",
    "\n",
    "> Hint: you can represent a *XOR* b as a combination of other logic ports: (a *NAND* b) *AND* (a *OR* b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29066aa4a84df0c9f84657184c023e47",
     "grade": true,
     "grade_id": "cell-cbe451bd8111af63",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# build empty network with appropriate inputs, hidden nodes, and outputs\n",
    "xorNN = NN(layer_sizes = [2, 2, 1])\n",
    "\n",
    "# set weight and biases for NAND, OR, and AND respectively\n",
    "xorNN.manually_set_weights([np.array([[-10, -10], [20, 20]]), \n",
    "                             np.array([[10, 10]])],\n",
    "                            [np.array([15, -10]),\n",
    "                             np.array([-15])])\n",
    "                            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdbcdc30e6a9745d5100c1c85222dbf0",
     "grade": false,
     "grade_id": "cell-1c76b2353c92b8e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee65e10198eb5dd3b335c2da7fbe2c71",
     "grade": true,
     "grade_id": "cell-3b5faaefbb75c43c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(round_output(xorNN.forward(X)), xorY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68a7271c74ddae1e3b40047f97e1c5dd",
     "grade": false,
     "grade_id": "cell-738f73c0f622a8f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Classifying handwritten digits\n",
    "\n",
    "This principle is at the *core* of neural networks; adding layers together can solve *much* more complicated problems than just using one layer. For the last part of this assignment, we'll try this algorithm on something a little more interesting than boolean functions, digit recognition.\n",
    "\n",
    "The data for this problem can be found in the file `digits123.csv`. Each row contains 65 values, where the first 64 are grayscale pixel values, and the last value is the class label, corresponding to the digit being shown. The grayscale values are integers ranging from 1 to 16, and using some reshaping, can be reconstructed back into a low-resolution *8x8* image.\n",
    "\n",
    "Below is the code to show the image for one digit, i.e. the input for one sample in the data set. Run the code to see the image. Currently, this is the sample at index 200 in the data set, but you can also change this index to display different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for this digit was: 2\n"
     ]
    }
   ],
   "source": [
    "digits = np.loadtxt('data/digits123.csv', delimiter=',', dtype=int)\n",
    "\n",
    "def display_digit(i, digits):\n",
    "    digit_sample = np.ones((8,8))*16 - np.reshape(digits[i, :-1], (8, 8))\n",
    "    plt.imshow(digit_sample, cmap='gray', vmax=16)\n",
    "    plt.show()\n",
    "    print(\"The label for this digit was:\", digits[i, -1])\n",
    "\n",
    "display_digit(200, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60331463378b0e09dbe2db5af21bc0a4",
     "grade": false,
     "grade_id": "cell-99cf5a79f07ec229",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Digit recognition solution\n",
    "\n",
    "Recognizing these very low-resolution digits, based on the individual pixel values, is probably not a task with a simple linear decision boundary. However, it is actually solvable with just one hidden layer!\n",
    "\n",
    "Learning the weights of a multi-layer neural network is also done using gradient descent, it is just that computing the correct partial derivatives for *all* the parameters is a lot more tricky. The algorithm to do this is called *backpropagation* and is something we will look at next module.\n",
    "\n",
    "For this assignment, you're just provided with the already learned weight matrices for this digit recognition problem. Running the code below will load the included matrix files and create a complete 2 layer network in `digitNN`. The network has 64 inputs, 65 hidden nodes, and 3 outputs, with each output corresponding to one of the 3 possible digits; 1, 2 and 3.\n",
    "\n",
    "For this last step, you will need to compute the accuracy of this network in predicting the digits in this data set. The network output will be a $542 \\times 3$ matrix, where the first column indicates the probability that sample was a $1$, the second column the probability the sample was a $2$ and the last column the probability that sample was a $3$. Ultimately, the model should classify each sample as whichever digit was *most likely*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d434692d7b6e8014f23ac8716921973",
     "grade": false,
     "grade_id": "cell-b798b56e722cc19b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 9\n",
    "\n",
    "Implement the function `compute_accuracy`, which takes a matrix of network outputs `Y_hat` and a one-hot encoded set of class labels `Y`, and returns the percentage of samples that was classified correctly by the network.\n",
    "\n",
    "*Hint:* The function [np.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) will save you a lot of work when applied correctly to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f011bbb8dcf426bf44e5c7488282475",
     "grade": true,
     "grade_id": "cell-81121b39559cc407",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The network accuracy for these digits was:\n",
      "99.63099630996311\n"
     ]
    }
   ],
   "source": [
    " # Normalize the values of the pixels to be between 0 and 1\n",
    "X = digits[:, :-1] / 16\n",
    "\n",
    "# Generate one-hot encoding for Y\n",
    "y = digits[:, -1]\n",
    "Y = np.eye(y.max())[y - y.min()]\n",
    "\n",
    "# Load the already backpropagated weights for the network\n",
    "digitNN = NN((64, 65, 3))\n",
    "L0 = np.load('data/digits_theta_1.npy')\n",
    "L1 = np.load('data/digits_theta_2.npy')\n",
    "digitNN.layers[0].b = L0[:,0]\n",
    "digitNN.layers[0].W = L0[:,1:]\n",
    "digitNN.layers[1].b = L1[:,0]\n",
    "digitNN.layers[1].W = L1[:,1:]\n",
    "\n",
    "def compute_accuracy(Y_hat, Y):\n",
    "    '''\n",
    "    This function takes as input two matrixes, prediction (with hypothesis values) and y (with ground-truth values)\n",
    "    The for loop counts the number of predictions in which both matrices have the same output.\n",
    "    similar output is established using the np.argmax.\n",
    "    '''\n",
    "    \n",
    "    # get the indices of the maximum values along the axis.\n",
    "    vector_Y_hat = np.argmax(Y_hat, axis = 1)\n",
    "    vector_Y = np.argmax(Y, axis = 1)\n",
    "    \n",
    "    # set counter to calculate accuracy\n",
    "    counter_correct = 0\n",
    "    counter_total = 0\n",
    "    \n",
    "    # loop over and compare vectors\n",
    "    for i in range(len(vector_Y_hat)):\n",
    "        counter_total += 1\n",
    "        if vector_Y_hat[i] == vector_Y[i]:\n",
    "            counter_correct += 1\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = counter_correct / counter_total * 100\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digitNN.forward(X)\n",
    "\n",
    "print(\"\\nThe network accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4d08b785cbd190764d632a4709ed2a7",
     "grade": false,
     "grade_id": "cell-c7ea7ab2f273a2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Learning\n",
    "\n",
    "Next week we're going to learn how you can train neural networks in general. But let's glance into the future and look at how we can train **a single** logistic layer. For single layer neural networks, learning the parameters, $W$ and $b$, is very similar to learning $\\theta$ with logistic regression.\n",
    "\n",
    "The single layer network takes a vector of inputs for a sample and produces a vector of predictions (instead of just one prediction per sample, as with logistic regression). This vector of predictions corresponds with multi-class classification, with each output learning a separate *one-vs-all* classifier; one classifier to recognize each of the classes that needs to be distinguished. This means that learning the parameters for the whole neural network will, in the case of a *single layer*, just correspond learning several logistic regression outputs \"next to each other\".\n",
    "\n",
    "#### As a computational graph\n",
    "\n",
    "Just as with logistic regression, we use'll the cross entropy loss $l$. However, since the output of the logistic layer is a vector of predictions, the equation for the cost will also have to change accordingly.  To determine the combined cost for all these one-vs-all classifiers, you can just sum the individual cost for each of the outputs together, and compute the total cost that way.\n",
    "\n",
    "At every training step we would like to calculate how much every weight in $W$ and $b$ should change to decrease this total loss. For this we need to compute the gradients $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$. In the computational graph below we added these gradients:\n",
    "\n",
    "<img src=\"src/Multi layer NN back.svg\" width=\"39%\">\n",
    "\n",
    "#### The math\n",
    "\n",
    "Again, since the output of the logistic layer is not a single value, but a vector, the equations for the gradients are also slightly different to those of logistic regression. If you work out the math, you will see that the difference boils down to doing an extra summation over the outputs. The details are for next week. For now, we will just provide you with the resulting equations:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{W,b}}{\\partial W} = \\frac{1}{m}(\\hat{Y} - Y)^T \\cdot X \\\\\n",
    "\\frac{\\partial J_{W,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial J_{W,b}}{\\partial b}$ is simply the sum of all the rows of the $m \\times o$ matrix $\\hat{Y} - Y$.\n",
    "\n",
    "You can verify for yourself that this is congruent with logistic regression by taking the output dimension $o = 1$ (i.e., you can take $Y$ and $\\hat{Y}$ to be vectors of size $m$ instead of $m \\times o$ matrices). If you work out the math for yourself, you will see that the above equations become identical to those of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c72bc97fc2cf379bf41865c35173274",
     "grade": false,
     "grade_id": "cell-463e91074827ac8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 10\n",
    "\n",
    "Use the formulas above to define the `LogisticLayer` class here below. The `init` is already given, you can copy the `forward` method from above, and you'll have to add the missing methods `backward`, `step`, and `cost`.\n",
    "\n",
    "Note that all these functions, so `forward`, `backward`, `step`, and `cost`, should work in exactly the same way as they do for the `Logistic` class from *Assignment 1*, except that they should implement these operations for the entire layer of outputs, instead of a single node. Specifically, each function should be implemented in such a way that they can be used by *same* `optimize` function from earlier, also used for the `Logistic` class, with only the model instance changing. \n",
    "\n",
    "So, most of these functions will need to store some intermediate results in the object instance, which can then be used by later function calls. For example, the `backward` function should store the passed `Y` matrix, making it possible to later compute the `cost` for that same `Y`, without needing to provide it as an argument. Take a good look at what arguments the functions from the `Logistic` class exactly take, and which attributes they store, and use that as a starting point for your `LogisticLayer` implementation.\n",
    "\n",
    "The implementation of `backward` only has to work for a single layer, for which the gradient equations are given above. You can reuse the code from logistic regression from earlier, but care should be taken in the vectorization. Since the there are now $o$ different outputs, the vectorized version will produce an $m \\times o$ matrix of predictions $\\hat{Y}$, which means ($\\hat{Y} - Y$) in the gradient computation is now also an $m \\times o$ matrix, instead of just a vector of size $m$. As a result, you'll need to make some changes in your implementation to keep all of the vectorized dimensions correct.\n",
    "\n",
    "> Note that the `init` method randomizes the $W$ and $b$ parameters. The reason for this will be discussed next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a4c955865f9ed9fc0182b7021893d52",
     "grade": true,
     "grade_id": "cell-a50659956d204831",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" set initial weights \"\"\"\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.W = np.random.randn(s_out, s_in)\n",
    "        self.b = np.random.randn(s_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.W, self.b and x.\"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        # z values\n",
    "        self.z = np.matmul(self.X, self.W.T) + self.b\n",
    "\n",
    "        # logistic function\n",
    "        self.Y_hat = 1 / (1 + np.exp(-self.z))\n",
    "        \n",
    "        return self.Y_hat\n",
    "    \n",
    "        \n",
    "    def backward(self, Y):\n",
    "        \"\"\" Compute gradients self.d_W and self.d_b, based on self.y_hat and y.\"\"\"\n",
    "        \n",
    "        self.Y = Y\n",
    "        \n",
    "        # set m to length of x\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        # calculate d_W\n",
    "        fraction = (1/m)\n",
    "        error = self.Y_hat - self.Y\n",
    "        error = np.matmul(error.T, self.X)\n",
    "        self.d_W = fraction * error\n",
    "        \n",
    "        # calculate d_b\n",
    "        fraction = (1/m)\n",
    "        sum_error = self.Y_hat - self.Y\n",
    "        self.d_b = fraction * sum_error\n",
    "\n",
    "        return self.d_W, self.d_b\n",
    "\n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" Update self.W and self.b based on self.d_W, self.d_b, and alpha.\"\"\"\n",
    "        \n",
    "        self.W = self.W - alpha * self.d_W\n",
    "        self.b = self.b - alpha * self.d_b\n",
    "        \n",
    "        return self.W, self.b\n",
    "        \n",
    "        \n",
    "        \n",
    "    def cost(self, Y = None):\n",
    "        \"\"\" Compute cost, based on prediction, self.y_hat, and target: y (or self.y).\"\"\"\n",
    "        \n",
    "        if not Y:\n",
    "            Y = self.Y\n",
    "            \n",
    "        # return cost based on:\n",
    "        # - the predicted output (self.y_hat) \n",
    "        # - and the actual values (y or self.y)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # set m to length of x\n",
    "        m = self.X.shape[0]\n",
    "        \n",
    "        # cost = -1 / m * np.sum(y * np.log(X) + (1 - y) * (np.log(1 - X)))\n",
    "        part1 = np.multiply(Y, np.log(self.Y_hat))\n",
    "        part2 = np.multiply((1 - self.Y), np.log(1 - self.Y_hat))\n",
    "        summ = np.sum(part1 + part2)\n",
    "        fraction = - (1 / m)\n",
    "        cost = (fraction * summ)\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1321071f3965e13631e28a71324d85bd",
     "grade": false,
     "grade_id": "cell-bcd5c2728f128f38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The test below optimizes the network using the function defined in the beginning of this notebook. If you defined the `LogisticLayer` correctly, it should learn the correct weights for the *AND* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2397ba379c6ca67c7864574a905942d9",
     "grade": true,
     "grade_id": "cell-b892e820789301a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 13312.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "testLL = LogisticLayer(2, 1)\n",
    "optimize(testLL, X, andY, 0.1, iterations = 1000)\n",
    "predictions = round_output(testLL.forward(X))\n",
    "print(predictions)\n",
    "\n",
    "np.testing.assert_array_equal(predictions, andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "509b1e78d0a2206747146c68a676952d",
     "grade": false,
     "grade_id": "cell-e0bec5ce089c7dea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 11\n",
    "\n",
    "Use the logistic layer to learn the correct weights for the *OR* output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "824f1015f05a8fde546ecae79628349a",
     "grade": true,
     "grade_id": "cell-c9b9b2ac4032cd5b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 14529.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Success!\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# similar to andLL, but now we work with the functions and classes for orLL\n",
    "orLL = LogisticLayer(2, 1)\n",
    "optimize(orLL, X, orY, 0.1, iterations = 1000)\n",
    "predictions = round_output(orLL.forward(X))\n",
    "print(predictions)\n",
    "\n",
    "np.testing.assert_array_equal(predictions, orY)\n",
    "print(\"Success!\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85cfc9b7a87099743f8eca1cdb59e2e4",
     "grade": true,
     "grade_id": "cell-2f71a408de0eb292",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(predictions, np.array([[0], [1], [1], [1]]))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

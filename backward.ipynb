{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68ebd7cfb555bcf90d544732faad1067",
     "grade": false,
     "grade_id": "cell-04bf4e28045fa30a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural networks and backpropagation\n",
    "\n",
    "In this module you will implement the backpropagation algorithm and use it to train neural networks on a couple of datasets. \n",
    "\n",
    "In the last module you learned how to generalize logistic regression for multiple outputs. Then you learned how to combine these generalized logistic regression units into a neural network. You've used the neural network to simulate logic gates and to recognize handwritten digits. \n",
    "\n",
    "In the last part of the module you saw how to train a single generalized logistic unit, which was very similar to training with logistics regression. The main part that you didn't implement was the training of a full neural network. This is the subject of this module. \n",
    "\n",
    "This requires two adaptations to what we've done last time. First we will need to revisit the equations for computing the gradients. It turns out that they are slightly different from what we saw last module. Second, you'll have to implement the backpropagation algorithm, that propagates the gradients back through the neural network.\n",
    "\n",
    "This module consists of four parts:\n",
    "\n",
    "0. We will start with a brief recap of the elements from last module on which we're going to continue to build. \n",
    "1. First you will adapt the LogisticLayer-class to use the revised equations for a logistic layer of a neural network.\n",
    "2. Then you will extend the NN-class to implement the backpropagation algorithm.\n",
    "3. Then you will train different neural networks on several datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52942eb2c74e74511ba2fa69facbcc20",
     "grade": false,
     "grade_id": "cell-f6f759b67bb03648",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 0: Recap\n",
    "\n",
    "Let's start by looking back at last module. Last module discussed some concepts that we will expand on in this module.\n",
    "\n",
    "### Generalized logistic regression\n",
    "\n",
    "Last module we saw how neural networks can be built out of logistic regression modules. We called such a module a `LogisticLayer`. \n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "You saw how to represent the logistic layer as a computational graph. This graph is a convenient visualization of the mathematics that make up a logistic layer.\n",
    "\n",
    "<img src=\"src/cg08_2.svg\" width=\"30%\">\n",
    "\n",
    "#### Forward \n",
    "\n",
    "\n",
    "The method `forward` of `LogisticLayer` implemented the prediction step using the same maths as for logistic regression, but generalized for more than one output:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z = A \\cdot W^T \\oplus b &&\n",
    "A_{\\mathrm{next}} = g(Z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Backward\n",
    "\n",
    "The method `backward` implemented the computation of the gradients of the `LogisticLayer`, using the equations below. **These are only correct for a single layer network!**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = (\\hat{Y} - Y)^T \\cdot X &&\n",
    "\\frac{\\partial l}{\\partial b} = \\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where, since there is only one layer, $X = A$ and $\\hat{Y} = A_{\\mathrm{next}}$. This module we will expand this set of equations to also work in the case of multi-layer networks.\n",
    "\n",
    "> Note that the computational graphs are often defined for single samples (i.e., lowercase `x`) whereas the equations are all given for multiple samples (i.e., uppercase `X`). This is to, on the one hand, conform to the description of computational graphs in descriptions of neural networks that you'll find elsewhere and, on the other hand, to provide you with equations that you can directly implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9f9749ab2ff6ccf881bf014e65e1c0c",
     "grade": false,
     "grade_id": "cell-3fee96a8fab78f2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Neural network (without learning)\n",
    "\n",
    "The class `NN` implemented a fully connected neural network. We worked out the forward pass for a neural network. We did not implement the backward pass for a multi-layer network yet. That will be the task for this module.\n",
    "\n",
    "The class `NN` used a series of `LogisticLayer` modules to build up the network. This way the `NN` class itself contains very little mathematics, most of that is delegated to the `LogisticLayer` class. Providing a clean, modular design.\n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "The computational graph of the forward pass of the $L$ layer neural network shows how the neural network is essentially $L$ logistic layers chained together. The output of logistic layer $i$ is the input of logistic layer $i+1$, and so the final output of layer $L$ depends on all the weight matrices $W_{1}$ to $W_{L}$ and all the bias terms $b_1$ to $b_L$ preceding it.\n",
    "\n",
    "<img src=\"src/cg07.svg\" width=\"100%\">\n",
    "\n",
    "\n",
    "#### Forward\n",
    "\n",
    "We implemented the `forward` method by iterating over all the logistic layers, calling the `forward` method of those and propagating the output forward.\n",
    "\n",
    "#### Loss\n",
    "\n",
    "The only mathematics that is not delegated to the logistic layers, but is implemented in the `NN` class directly, is that of the loss. The loss $l$ over the prediction (of a single sample) $\\hat{y}$ and the target $y$ is given by the cross entropy:\n",
    "\n",
    "$$\n",
    "l = - y \\cdot \\ln(\\hat{y}) - (1 - y) \\cdot \\ln(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "The total cost (over all samples) is defined by:\n",
    "\n",
    "$$\n",
    "J = \\sum_i^m l^i\n",
    "$$\n",
    "\n",
    "#### Backward\n",
    "\n",
    "The training of multi-layer neural networks is the main subject of this module. Last module we ended with the training of only a single logistic layer. Training a whole network is slightly more complex and requires us to implement backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7849279410b3b6fde0b9543cc174d91",
     "grade": false,
     "grade_id": "cell-b3a26924e5a2c588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Revised equations\n",
    "\n",
    "So what changes when we want to train a fully connected neural network? As mentioned above, we need to change the equations for computing the gradients and we need to implement backpropagation. In this part we will implement and test the new equations as part of the `LogisticLayer` class. In the next part we will implement the backpropagation algorithm as part of the `NN` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77c51e8bbc1638af0239312bedf2c5f6",
     "grade": false,
     "grade_id": "cell-c603a536c865cc9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Full computational graph of neural network\n",
    "\n",
    "When we add the gradients to the computational graph we get the following:\n",
    "\n",
    "<img src=\"src/cg10.svg\" width=\"100%\">\n",
    "\n",
    "What we want to compute in the backward step is the gradient for each $W_i$ and $b_i$ in the network. The difficulty is that these gradients cannot be computed in isolation. It turns out that, when you work out the mathematics, the gradients in layer $i$ are dependent on the gradient of the next layer ($i+1$). In other words, in order to be able to compute $\\frac{\\partial l}{\\partial W_1}$ and $\\frac{\\partial l}{\\partial b_1}$, we need to first compute $\\frac{\\partial l}{\\partial A_2}$, and in order to compute $\\frac{\\partial l}{\\partial A_2}$ (and $\\frac{\\partial l}{\\partial W_2}$ and $\\frac{\\partial l}{\\partial b_2}$) we need to first compute $\\frac{\\partial l}{\\partial A_3}$, etc. until we reach $\\frac{\\partial l}{\\partial A_L} = \\frac{\\partial l}{\\partial \\hat{Y}}$, which we can compute directly.\n",
    "\n",
    "So, and this is crucial, we need to compute the gradient of the final (output) layer first and then _work our way backwards_ through the graph. Hence, the term _backpropagation_. This is also indicated by the direction of the arrow of the gradients. The backpropagation algorithm will be discussed more formally later. But let's have a look at the order of computation to get an intuition:\n",
    "\n",
    "* The first thing we need to compute is the gradient of the output ($\\hat{Y}$): $\\frac{\\partial l}{\\hat{Y}}$. \n",
    "* With that we can compute the gradients in the final layer: $\\frac{\\partial l}{\\partial W_L}$, $\\frac{\\partial l}{\\partial b_L}$, and $\\frac{\\partial l}{\\partial A_L}$. \n",
    "* This gradient $\\frac{\\partial l}{\\partial A_L}$ is then used to compute the gradients $\\frac{\\partial l}{\\partial W_{L-1}}$, $\\frac{\\partial l}{\\partial b_{L-1}}$, and $\\frac{\\partial l}{\\partial A_{L-1}}$. \n",
    "* Then the gradient $\\frac{\\partial l}{\\partial A_{L-1}}$ is used to compute the gradients of the layer before that, etc., _propagating_ gradients all the way _back_ through the network until we reach the first layer.\n",
    "\n",
    "#### Full computational graph of single layer\n",
    "\n",
    "If we zoom in on a single layer we get the computational below. In the forward step we got the value $A$ as input and we computed the value for $A_{next}$. In the backward step we get the gradient $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ as input and we need to determine the gradients $\\frac{\\partial l}{\\partial W}$, $\\frac{\\partial l}{\\partial b}$, and (crucially for the backpropagation to work) $\\frac{\\partial l}{\\partial A}$.\n",
    "\n",
    "<img src=\"src/cg12.svg\" width=\"30%\">\n",
    "\n",
    "> Note that for the final layer, $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ is equal to $\\frac{\\partial l}{\\partial \\hat{Y}}$. For all the layers before that, $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$ is the gradient computed in the following layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e53b8bc885b6de505247620541614d8",
     "grade": false,
     "grade_id": "cell-d7cbf21666190ced",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradients\n",
    "\n",
    "So how do we concretely compute these gradients? Here you will see that the equations differ slightly from what you've seen before. Of course, there is the gradient $\\frac{\\partial l}{\\partial A}$, which we never had to compute before. But, also the gradients $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$ are computed slightly differently. They are given by the following equations (with $D$ defined below):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A &&\n",
    "\\frac{\\partial l}{\\partial b} = \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "\\frac{\\partial l}{\\partial A} = D \\cdot W\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $D$ is a common term in all the gradients given by the following equation:\n",
    "\n",
    "$$\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "$$\n",
    "\n",
    "Again, the $\\odot$ and $\\ominus$ are not linear algebra operations. They do correspond to simple multiplication and subtraction in Numpy ($*$ and $-$). You could also work out the $D$ term in proper linear algebra operations, but it wouldn't be very helpful as you would have to translate them back into these Numpy operations when implementing the equation for $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df05ac35a90bd84bc4cfaea9f984bcce",
     "grade": false,
     "grade_id": "cell-6079a46c74f58992",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation of LogisticLayer\n",
    "\n",
    "We already implemented the `LogisticLayer` class in the previous module. However, we implemented the backward step using the simplified (old) equations, since we knew that the layer was always the output layer (it was the only layer). Now we need to adapt the `backward` method of the `LogisticLayer` class to use the new equations instead, as the new equations work for all layers, not only for a single layer as the old equations did. So, repeated from above, this is the computational graph that the `LogisticLayer` class implements:\n",
    "\n",
    "<img src=\"src/cg12.svg\" width=\"30%\">\n",
    "\n",
    "The `forward` method is already implemented for you. They use the same equations as you used in the previous module: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z = A \\cdot W^T \\oplus b &&\n",
    "A_{\\mathrm{next}} = g(Z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You still need to update the `backward` method to compute the gradients using the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A &&\n",
    "\\frac{\\partial l}{\\partial b} = \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "\\frac{\\partial l}{\\partial A} = D \\cdot W &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It should be noted that we're getting a bit ahead of ourselves: The computation of $\\frac{\\partial l}{\\partial A}$ is already part of the implementation backpropagation, but since it must be computed in the `LogisticLayer` class, it makes sense to deal with it now.\n",
    "\n",
    "### Assignment 1a\n",
    "\n",
    "Implement the `backward` method of the `LogisticLayer` class below. Bear in mind:\n",
    "* The input `DA_next` corresponds to $\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}$\n",
    "* `self.DW` and `self.Db` correspond to $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$, respectively.\n",
    "* `self.DA` corresponds to $\\frac{\\partial l}{\\partial A}$. \n",
    "* The method should return `self.DA`. As, we will see later, the backpropagation algorithm will rely on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f362d04c1766c01e2b843a40047e895",
     "grade": true,
     "grade_id": "cell-3e5e776052f00837",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.seterr(all='raise')\n",
    "\n",
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" set initial weights \"\"\"        \n",
    "        # init the learning parameters\n",
    "        self.W = np.random.randn(s_out, s_in)*0.6 - 0.3\n",
    "        self.b = np.random.randn(s_out)*0.6 - 0.3\n",
    "    \n",
    "        # init the gradients (later computed in backward)\n",
    "        # the gradient of W\n",
    "        self.DW = np.zeros(self.W.shape)\n",
    "        \n",
    "        # the gradient of b\n",
    "        self.Db = np.zeros(self.b.shape)\n",
    "        \n",
    "        # the gradient of input A (for backpropagation)\n",
    "        self.DA = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\" computes the ouput for this layer (A_next) \n",
    "            based on input A, and parameters W and b. \"\"\"\n",
    "        self.A = A\n",
    "        self.Z = np.matmul(A, self.W.T) + self.b\n",
    "        \n",
    "        self.A_next = 1 / (1 + np.exp(-self.Z) + 1e-10)\n",
    "        return self.A_next\n",
    "    \n",
    "    \n",
    "    def backward(self, DA_next):\n",
    "        \"\"\" compute the layer gradients based on the gradient w.r.t.\n",
    "            the next layer activation (DA_next), and the input (A)\n",
    "            and corresponding output (A_next) as computed in the\n",
    "            prediction step (forward). The method returns DA (the\n",
    "            gradient w.r.t. A). \"\"\"\n",
    "        \n",
    "        self.DA_next = DA_next\n",
    "        \n",
    "        D = self.A_next * (1 - self.A_next) * self.DA_next\n",
    "        \n",
    "        # the gradient of A\n",
    "        self.DA = np.matmul(D, self.W)\n",
    "        \n",
    "        # the gradient of W\n",
    "        self.DW = np.matmul(D.T, self.A)\n",
    "        \n",
    "        # the gradient of b\n",
    "        self.Db = D.sum(axis = 0)\n",
    "        \n",
    "        return self.DA\n",
    "        \n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" update the learning parameters W and b based on the \n",
    "            gradients (DW and Db) as computed in the backward step \"\"\"\n",
    "        self.b -= alpha * self.Db\n",
    "        self.W -= alpha * self.DW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b744639d99e7c6c0d3f215a8d3f10ed2",
     "grade": false,
     "grade_id": "cell-2674e78df2a1ec50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1b:\n",
    "\n",
    "Test the logistic layer. Try to think of ways to test the logistic layer yourself. What output do you expect for what input?\n",
    "\n",
    "For example, if the dimensions of the gradient $\\frac{\\partial l}{\\partial A}$ should be the same as the dimensions of the input $A$. The same is true for the dimensions of $\\frac{\\partial l}{\\partial W}$ and $W$ and for the dimensions of $\\frac{\\partial l}{\\partial b}$ and $b$. You can test if this is the case.\n",
    "\n",
    "Another example: if $\\frac{\\partial l}{\\partial A_{next}}$ contains only 0's, it means that the prediction is identical to the target. In such a case we should be at a (local) minimum and, if we implemented the model correctly, all other gradients should also be (close to) 0.\n",
    "\n",
    "Discuss with your fellow students what other ways you could test if your implementation is correct, and write at least two tests down below.\n",
    "\n",
    "*Note:* Implementing the suggested tests from the text above would be sufficient for this assignment. However, trying to think of your own tests can be a very useful exercise, so you are encouraged to do so and discuss your ideas with your fellow students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f17f83e11a2926280c05024b32524e57",
     "grade": true,
     "grade_id": "cell-af6223b91c2a68e9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# pick simple network structure\n",
    "s_in = 4\n",
    "s_out = 2\n",
    "ll = LogisticLayer(s_in, s_out)\n",
    "\n",
    "# run LL for prediction and gradients\n",
    "prediction = ll.forward(np.array([[0.9, 1.0, 1.1, 1.2], [1.3, 1.4, 1.5, 1.6]]))\n",
    "gradient = ll.backward(np.array([[0.1, 0.2], [0.3, 0.4]]))\n",
    "\n",
    "# check dimensions of gradients\n",
    "assert ll.DA.shape ==  (ll.A).shape\n",
    "assert ll.DW.shape ==  (ll.W).shape\n",
    "assert ll.Db.shape ==  (ll.b).shape\n",
    "\n",
    "\n",
    "# pick simple network structure\n",
    "s_in = 2\n",
    "s_out = 1\n",
    "ll = LogisticLayer(s_in, s_out)\n",
    "\n",
    "# create testing values\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# apply forward\n",
    "Y_test = ll.forward(X_test)\n",
    "\n",
    "# apply backward to output values of zero with shape Y_test\n",
    "ll.backward(np.zeros_like(Y_test))\n",
    "\n",
    "# create matrices with only zeros for comparison in assertions\n",
    "DA_zeros = np.zeros_like(ll.DA)\n",
    "DW_zeros = np.zeros_like(ll.DW)\n",
    "Db_zeros = np.zeros_like(ll.Db)\n",
    "\n",
    "# check whether all gradients are close to zero\n",
    "assert np.allclose(ll.DA, DA_zeros, atol = 1e-06), \"error DA\"\n",
    "assert np.allclose(ll.DW, DW_zeros, atol = 1e-06), \"error DW\"\n",
    "assert np.allclose(ll.Db, Db_zeros, atol = 1e-06), \"error Db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Explain why you chose these particular tests and for each why they have that expected output.**\n",
    "\n",
    "*I chose to test as explained in the examples above.\n",
    "\n",
    "It makes sense to test the dimensions of the gradients because the dimensions of the gradients should be the same as the dimensions of the inputs and of the weights and biases.\n",
    "\n",
    "It also makes sense to check whether the gradients are close to zero if we are at a local minimum. In order to do this, I created some simple network structure and testing values. And I applied forward and backward. For this, its important that backwards is applied to a matrix with the shape op Y_test (relates to X_test) that consists of zeros. Then, the gradients should be zero and this is what we check in the asserts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92685bedbc25f7658d9e63303d436b9c",
     "grade": false,
     "grade_id": "cell-f798180659494896",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After testing the LogisticLayer class yourself, use the test below to see if you get the same answers as us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c58f061663e24f6230f37d31a022534f",
     "grade": true,
     "grade_id": "cell-8287c9c8b9ce3175",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test LogisticLayer: succes!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test LogisticLayer: \", end = '')\n",
    "\n",
    "ll = LogisticLayer(3,2)\n",
    "ll.W = np.array([[0.1, 0.2, 0.3], [0.4,0.5,0.6]])\n",
    "ll.b = np.array([0.7,0.7])\n",
    "prediction = ll.forward(np.array([[0.9, 1.0, 1.1], [1.2, 1.3, 1.4]]))\n",
    "gradient = ll.backward(np.array([[0.1, 0.2], [0.3, 0.4]]))\n",
    "\n",
    "_prediction, _gradient = [[0.789, 0.902], [0.818, 0.935]], [[0.009, 0.012, 0.016], [0.014, 0.021, 0.028]]\n",
    "_DW, _Db = [[0.069, 0.075, 0.081], [0.045, 0.049, 0.053]], [0.061, 0.042]\n",
    "\n",
    "np.testing.assert_almost_equal(prediction, _prediction, decimal = 2)\n",
    "np.testing.assert_almost_equal(gradient, _gradient, decimal = 2)\n",
    "np.testing.assert_almost_equal(ll.DW, _DW, decimal = 2)\n",
    "np.testing.assert_almost_equal(ll.Db, _Db, decimal = 2)\n",
    "\n",
    "\n",
    "print(\"succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6968f694c39932899442924690631e0",
     "grade": false,
     "grade_id": "cell-fadc2888c87c444e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Backpropagation\n",
    "\n",
    "With the `LogisticLayer` in place, we're almost there. We now need to connect them all up again into a neural network as we did in last module. But, this time we're going to also do the backward step. Have another look at the computational graph:\n",
    "\n",
    "<img src=\"src/cg10.svg\" width=\"100%\">\n",
    "\n",
    "We've implemented everything that's inside the green rectangles. Now we can focus on the transitions between them. First we need to compute the gradient of the output, $\\frac{\\partial l}{\\partial \\hat{Y}}$. And then we can use the backpropagation algorithm to propagate the gradients $\\frac{\\partial l}{\\partial A_{i}}$ back through the network. \n",
    "\n",
    "First we will look at the $\\frac{\\partial l}{\\partial \\hat{Y}}$, then the backpropagation algorithm.\n",
    "\n",
    "#### The gradient of the output\n",
    "\n",
    "So for backpropagation to work, the first thing we need to determine is the gradient for the output $Y$. For the output of a single sample ($\\hat{y}$), the loss $l$ is the same cross entropy we've seen before:\n",
    "\n",
    "$$\n",
    "l = -y \\cdot \\ln(\\hat{y}) - (1 - y) \\cdot \\ln(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "And the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{y}} = \\frac{\\partial}{\\partial \\hat{y}} \\left( -y \\cdot \\ln(\\hat{y}) - (1 - y) \\cdot \\ln(1 - \\hat{y}) \\right)\n",
    "$$\n",
    "\n",
    "This works out to be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{y}} = \\frac{- y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}\n",
    "$$\n",
    "\n",
    "In terms of matrices (multiple samples) this becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{Y}} = \\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})\n",
    "$$\n",
    "\n",
    "Again, the $\\ominus$ and $\\oslash$ are not linear algebra operations (just like $\\odot$, $\\oplus$ and $\\ominus$ before). The operators $\\ominus$ and $\\oslash$ correspond to simple subtraction and division in Numpy ($-$ and $/$). Also here, it wouldn't be helpful to express the $\\frac{\\partial l}{\\partial \\hat{Y}}$ term in true linear algebra, as the current notation is much closer to how you will implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c0586f1d837baffcd3de52fcc1b7b45",
     "grade": false,
     "grade_id": "cell-59298e8c6756c0e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Backpropagation pseudo code.\n",
    "\n",
    "With the $\\frac{\\partial l}{\\partial \\hat{Y}}$ term defined we can look at the final step: backpropagation. As mentioned earlier, the idea of backpropagation is that you start computing the gradients in the last layer and work your way back through the neural network. With that in mind, have a look at the pseudo code for backpropagation:\n",
    "\n",
    "\n",
    "---\n",
    "1. Run the forward pass and predict: $\\hat{Y}$\n",
    "2. Compute the gradient for the output: $\\frac{\\partial l}{\\partial \\hat{Y}}$\n",
    "3. Set a variable `DA`$:= \\frac{\\partial l}{\\partial \\hat{Y}}$\n",
    "4. Loop for layer in logistic layers i from L to 1 (i.e., backwards)\n",
    "    1. Compute backward pass for logistic layer i (using `layer.backward(DA)`):\n",
    "        1. Compute gradient $\\frac{\\partial l}{\\partial W}$\n",
    "        2. Compute gradient $\\frac{\\partial l}{\\partial b}$\n",
    "        3. Compute gradient $\\frac{\\partial l}{\\partial A}$\n",
    "        4. Return $\\frac{\\partial l}{\\partial A}$\n",
    "    2. Set `DA`$:= \\frac{\\partial l}{\\partial A}$\n",
    "---\n",
    "\n",
    "In this algorithm, steps (a, b, c, and d) are already implemented in the `LogisticLayer` class. So the only thing you have to do is call the `backward()` method of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5d3f4f03f0892485faef3e2e5164e31",
     "grade": false,
     "grade_id": "cell-677b557e29ce6feb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2a\n",
    "\n",
    "The neural network below is mostly implemented. Only the `backward` method, implementing the backpropagation algorithm, is missing. Implement this method. \n",
    "\n",
    "Keep in mind that the only maths that you still have to implement in the `backward` method is that for $\\frac{\\partial l}{\\partial \\hat{Y}}$. The rest of the mathematics is delegated to the `LogisticLayer` objects that are stored in `self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5997ad4245ac4bdd285516d742bb040",
     "grade": true,
     "grade_id": "cell-87c3ec7cd6d6d29a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layer_sizes = [2,2,1]):\n",
    "        \"\"\" Set initial layers. \"\"\"\n",
    "        self.layers = []\n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(LogisticLayer(s_in, s_out))\n",
    "            \n",
    "    def manually_set_weights(self, Ws, bs):\n",
    "        \"\"\" Provide list of weight matrices Ws and list of bias vectors bs. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert len(Ws)==len(self.layers), \"Ws: wrong length\"\n",
    "        assert len(bs)==len(self.layers), \"bs: wrong length\"\n",
    "        \n",
    "        for layer, W, b in zip(self.layers, Ws, bs):\n",
    "            layer.manually_set_weights(W, b)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction for X based on self.layers\"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        self.Y_hat = A\n",
    "        return self.Y_hat\n",
    "   \n",
    "    def backward(self, Y):\n",
    "        self.Y = Y\n",
    " \n",
    "        # Set variable DA\n",
    "        self.DA = - self.Y / self.Y_hat + (1 - self.Y) / (1 - self.Y_hat)\n",
    "        \n",
    "        # compute backward for layer and set DA\n",
    "        for layer in reversed(self.layers):\n",
    "            self.DA = layer.backward(self.DA)\n",
    "        \n",
    "        return self.DA\n",
    "        \n",
    "    \n",
    "    def cost(self, Y = None):\n",
    "        if not Y:\n",
    "            Y = self.Y\n",
    "        j = - Y * np.log(self.Y_hat) - (1 - Y) * np.log(1 - self.Y_hat)\n",
    "        cost = np.sum(j) / len(Y)\n",
    "        return cost\n",
    "    \n",
    "    def step(self, alpha = 0.1):\n",
    "        for layer in self.layers:\n",
    "            layer.step(alpha = alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ff7c56204f82d5083b2606d9b8dba75",
     "grade": false,
     "grade_id": "cell-b64ba8c731405514",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2b\n",
    "\n",
    "Again, try to think of ways you can test the correct working of the `NN` class yourself. Some ideas to start with: What should be the value of the gradients if the target and value are the same? What should be the dimensions of the output if we define a neural network with `layer_sizes = [3,2,1]` and an input matrix of shape `(18, 3)`? What should the dimensions of all the gradients be?\n",
    "\n",
    "Discuss with your fellow students what other ways you could test if your implementation is correct, and write at least two tests down below.\n",
    "\n",
    "*Note:* Implementing the suggested tests from the text above would be sufficient for this assignment. However, trying to think of your own tests can be a very useful exercise, so you are encouraged to do so and discuss your ideas with your fellow students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a7627c6948e0147a16ccd229b7fb71",
     "grade": true,
     "grade_id": "cell-a1b400452aa81b8d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. check gradient A\n",
    "\n",
    "# pick simple network structure\n",
    "nn = NN(layer_sizes = [2,2,1])\n",
    "\n",
    "# create testing values\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# apply forward\n",
    "Y_test = nn.forward(X_test)\n",
    "\n",
    "# apply backward to output values of zero with shape Y_test\n",
    "DA = nn.backward(Y_test)\n",
    "\n",
    "# create matrices with only zeros for comparison in assertions\n",
    "DA_zeros = np.zeros_like(nn.DA)\n",
    "\n",
    "# check whether all gradients are close to zero\n",
    "assert np.allclose(DA, DA_zeros, atol = 1e-06), \"error DA\"\n",
    "\n",
    "\n",
    "# 2. check shape DA:\n",
    "\n",
    "# pick simple network structure\n",
    "nn = NN(layer_sizes = [3, 2, 1])\n",
    "\n",
    "# init the learning parameters\n",
    "X_test_random = np.random.randn(18, 3)\n",
    "\n",
    "# apply forward\n",
    "Y_test_rand = nn.forward(X_test_random)\n",
    "\n",
    "# apply backward to output values of zero with shape Y_test\n",
    "DA = nn.backward(Y_test_rand)\n",
    "\n",
    "assert DA.shape == (18, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Explain why you chose these particular tests and for each why they have that expected output.**\n",
    "\n",
    "*Fist check is to check the gradient of A. This is quite similar to the check applied in 1b. \n",
    "Second check is to check the dimension of DA. This is the expected output because the shape of DA should indeed be the same as x_test_random because we execute a forward loop and a backward loop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaebc4ac45fc4674dcc41446c0afa228",
     "grade": false,
     "grade_id": "cell-d285399505bd7325",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After testing the `NN` class yourself, use the test below to see if you get the same answers as us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0783c296970f3e759dba21dce6a535df",
     "grade": true,
     "grade_id": "cell-224e3df5eed5a1a4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NN: succes!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NN: \", end = '')\n",
    "\n",
    "np.random.seed(0)\n",
    "nn = NN([3,2,1])\n",
    "prediction = nn.forward(np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]))\n",
    "target = np.array([[1], [-1]])\n",
    "nn.backward(target)\n",
    "\n",
    "\n",
    "_prediction = [[0.3885], [0.3817]]\n",
    "np.testing.assert_almost_equal(prediction, _prediction, decimal = 2)\n",
    "\n",
    "_gradients = [{'DW': [[-0.0388, -0.0445, -0.0503], [-0.0066, -0.0077, -0.0087]], \n",
    "               'Db': [-0.0573, -0.0106], 'DA': [[0.0484, 0.0033, 0.0082], [-0.103, -0.0086, -0.0153]]}, \n",
    "              {'DW': [[0.559, 0.4111]], 'Db': [0.7702], 'DA': [[0.2213, 0.0328], [-0.5001, -0.0741]]}]\n",
    "for layer, _answer in zip(nn.layers, _gradients):\n",
    "    np.testing.assert_almost_equal(layer.DW, _answer['DW'], decimal = 2)\n",
    "    np.testing.assert_almost_equal(layer.Db, _answer['Db'], decimal = 2)\n",
    "    np.testing.assert_almost_equal(layer.DA, _answer['DA'], decimal = 2)\n",
    "\n",
    "\n",
    "print(\"succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a2674ad0eb34d96605f06bd12314d43",
     "grade": false,
     "grade_id": "cell-363c2f5f47eaafdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Intermezzo: Why did the equations change?\n",
    "\n",
    "This part doesn't contain any assignments, but you should read it as it will help you understand what's going on.\n",
    "\n",
    "You might be confused or surprised by the maths. Why is the math for a multi layer network different from the math of the single layer network that we've seen in the previous module? Of course, for a single layer you don't have to compute the gradient for $A$ since there is no backpropagation, but you would still expect the gradients for $W$ and $b$ to be described by the same equations.\n",
    "\n",
    "However, these are the equations for a single logistic layer, as implemented in the previous module:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = (\\hat{Y} - Y)^T \\cdot X && \\frac{\\partial l}{\\partial b} = \\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And these are the equations for a layer in a neural network, as implemented in this module (let's call these the *standard equations*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A && \\frac{\\partial l}{\\partial b} = \\sum_i^m D^{\\mathrm{row}=i} &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\frac{\\partial l}{\\partial A_{\\mathrm{next}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can see that both sets of equations have a similar form, but they are clearly not the same. How come? This is a subtle, but important, point that is often omitted in explanations of neural networks: If we assume that the layer of the network is the final layer, then the equations _are actually equivalent_. \n",
    "\n",
    "Why? If the layer is the last layer, then by definition:\n",
    "\n",
    "$$\n",
    "A_{\\mathrm{next}} = \\hat{Y}\n",
    "$$\n",
    "\n",
    "This changes the last equations of the final layer:\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} \\odot(1 \\ominus \\hat{Y}) \\odot \\frac{\\partial l}{\\partial \\hat{Y}}\n",
    "$$\n",
    "\n",
    "where we can substitute the definition of the derivative of the loss w.r.t. the predicted output:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial \\hat{Y}} = \\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})\n",
    "$$\n",
    "\n",
    "resulting in\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} \\odot(1 \\ominus \\hat{Y}) \\odot [\\ominus Y\\oslash \\hat{Y} + (1\\ominus Y)\\oslash (1\\ominus \\hat{Y})]\n",
    "$$\n",
    "\n",
    "Which, if we work out the math and let the dust settle, reduces to:\n",
    "\n",
    "$$\n",
    "D = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "> You can verify this for yourself. Since none of the above operations are linear algebra operations you can translate the equation back to a normal algebra equation:\n",
    "$$\n",
    "d= \\hat{y} \\cdot (1 - \\hat{y}) \\cdot [-y / \\hat{y} + (1 - y) / (1 - \\hat{y})]\n",
    "$$\n",
    "you can work out for yourself that this indeed reduces to:\n",
    "$$\n",
    "d = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "If we now fill in $D = \\hat{Y} - Y$ into the original equation we get:\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A = (\\hat{Y} - Y)^T \\cdot A\\\\ \n",
    "\\frac{\\partial l}{\\partial b} = \\sum_i^m D^{\\mathrm{row}=i} = \\sum_i^m (\\hat{Y} - Y)^{\\mathrm{row}=i}\n",
    "$$\n",
    "\n",
    "Which should look very familiar!\n",
    "\n",
    "So, as we can see the equations for the final layer of a neural network are equivalent to those for the single Logistic Regression module. The problem is that most layers are not the final layer. And for those, this simplification doesn't work. So in general we are stuck with the more complex standard equations.\n",
    "\n",
    "In principle we could make a special case for the final layer. We could define one layer which is a logistic layer combined with the cost function, so we can use the simplified equations for that one layer and use the standard equations for all the other layers. This would be a bit more efficient (saving some costly operations) and it has some technical advantages (saving us from some potential rounding errors). You will see in the future that creating a special final layer is quite common practice, but we do not do this in this module. Using the same (standard) equations for all layers gives us a cleaner implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "701658b5b8727640c3d945ff2f26fb44",
     "grade": false,
     "grade_id": "cell-88ed2e3f8b6a2372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Training the neural network\n",
    "\n",
    "Now it's time to really put our neural network to the test! First let's start with some functions from previous module. \n",
    "\n",
    "As you might see, the `optimize` is identical to previous module. There we used it for logistic regression, but since all gradient descent algorithms work the same and we've defined the same methods (`forward()`, `backward()`, `step()`, and `cost()`), we can use the exact same function for training a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d620a41ce01ece67cc2351e7e9d79c9c",
     "grade": false,
     "grade_id": "cell-19e1c4ac033f34e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(model, X, y, alpha, iterations = 500, desc = 'iteration'):\n",
    "    \"\"\"Apply gradient descent to any model that inherits the Module class.\"\"\"\n",
    "    costs = []\n",
    "    for i in tqdm(range(iterations), leave=False, desc = desc):\n",
    "        # descent\n",
    "\n",
    "        model.forward(X)        \n",
    "        model.backward(y)\n",
    "        model.step(alpha)\n",
    "        \n",
    "        # keep track of costs\n",
    "        costs.append(model.cost())\n",
    "        \n",
    "        # check for divergence (alpha too big)\n",
    "        if len(costs) >= 2 and (costs[-2] - costs[-1]) < 0:\n",
    "            print(f'Diverging at iteration {len(costs)}')\n",
    "            return costs  \n",
    "    return costs\n",
    "\n",
    "def plot_costs(costs):\n",
    "    plt.title(\"The development of the cost of the model\")\n",
    "    plt.plot(range(len(costs)), costs)\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix(p, y):\n",
    "    return np.matmul(np.vstack((p, 1 - p)), np.vstack((y, 1 - y)).T)\n",
    "\n",
    "def round_output(x):\n",
    "    return (x >= 0.5) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "860ab30c1c23c48ae21dd6ad613acab8",
     "grade": false,
     "grade_id": "cell-7a3b4c6a2fbc54bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 1: Logic gates*\n",
    "\n",
    "The code below uses your neural network to learn a logic *and*. If you implemented the neural network correctly, it should work as is.\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "While the code below works, maybe you can get it to learn a bit faster. Play around with the learning rate and try to minimize the number of required iterations. Make sure you still reach a good optimum for the network parameters and don't end up with any false positives or false negatives in your predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.  0.]\n",
      " [ 0. 60.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "predictions = np.zeros((0,1))\n",
    "target = np.zeros((0,1))\n",
    "\n",
    "## run NN 20 times to make sure it always works (no matter the initialization of the learning parameters)\n",
    "for i in range(20):\n",
    "    testNN = NN([2, 2, 1])\n",
    "    costs = optimize(testNN, X, andY, 0.1, iterations = 350, desc = f'trial {i:2}')\n",
    "    _predictions = round_output(testNN.forward(X))\n",
    "    \n",
    "    # store results for comparison\n",
    "    predictions = np.append(predictions, _predictions, axis = 0)\n",
    "    target = np.append(target, andY, axis = 0)\n",
    "\n",
    "print(confusion_matrix(predictions[:,0], target[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. What is the minimum amount of iterations you needed to correctly learn this data?**\n",
    "\n",
    "*Alpha = 1.5\n",
    "Iterations = 8.\n",
    "\n",
    "It's really fast this way. I would normally not chose an alpha that is this big, but there do not seem to be any false positives and the algorithm does not diverge. At an alpha of 2, the algorithm diverges, so that's too big. 1.5 and 8 seem good and its really fast. *\n",
    "\n",
    "**Q4. Add a hidden layer of size 2 to the network. Does this change the optimal learning rate and number of iterations? Explain how.**\n",
    "\n",
    "*Since we added a hidden layer, we have many more weights in the process which makes the algorithm more complex. So, it makes sense that the programme needs more iterations and a smaller learning rate in order to take all of these influences into account.\n",
    "\n",
    "The hidden layer also has more nodes than the first layer, which also increases the overall complexity*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "343bfddf6e4996fbfcae26d8c33cd090",
     "grade": false,
     "grade_id": "cell-0f7245bdc8d23767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4\n",
    "\n",
    "The code below should learn a logical *xor*, but it doesn't work. Fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40.  0.]\n",
      " [ 0. 40.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "predictions = np.zeros((0,1))\n",
    "target = np.zeros((0,1))\n",
    "\n",
    "## run NN 20 times to make sure it always works (no matter the initialization of the learning parameters)\n",
    "for i in range(20):\n",
    "    testNN = NN([2, 10, 1])\n",
    "    costs = optimize(testNN, X, xorY, 0.08, iterations = 900, desc = f'trial {i:2}')\n",
    "\n",
    "    _predictions = round_output(testNN.forward(X))\n",
    "    \n",
    "    # store results for comparison\n",
    "    predictions = np.append(predictions, _predictions, axis = 0)\n",
    "    target = np.append(target, xorY, axis = 0)\n",
    "\n",
    "print(confusion_matrix(predictions[:,0], target[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Explain what you did to fix this, and why that was necessary.**\n",
    "\n",
    "*I added some nodes to the second layer and adjusted the number of iterations and the learning rate. This is necessary for an XOR because an additional layer allows for the algorithm to learn a more complicated feature like XOR logic.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12f7dd2ebd7ece84381a50dff51e6b95",
     "grade": false,
     "grade_id": "cell-ea3dc8d29f7872c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 2: Digits*\n",
    "\n",
    "Last module you got a pre-trained neural network for recognizing handwritten digits 1, 2 and 3. Now it's time to train this ourselves. First, load the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for this digit was: 2\n"
     ]
    }
   ],
   "source": [
    "digits = np.loadtxt('data/digits123.csv', delimiter=',', dtype=int)\n",
    "\n",
    "def display_digit(i, digits):\n",
    "    digit_sample = np.ones((8,8))*16 - np.reshape(digits[i, :-1], (8, 8))\n",
    "    plt.imshow(digit_sample, cmap='gray', vmax=16)\n",
    "    plt.show()\n",
    "    print(\"The label for this digit was:\", digits[i, -1])\n",
    "\n",
    "display_digit(200, digits)\n",
    "\n",
    "# Normalize the values of the pixels to be between 0 and 1\n",
    "X = digits[:, :-1] / 16\n",
    "\n",
    "# Generate one-hot encoding for Y\n",
    "y = digits[:, -1]\n",
    "Y = np.eye(y.max())[y - y.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7daf17edd7018a1f7a6f04057c39d08",
     "grade": false,
     "grade_id": "cell-42522856d9190ed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5\n",
    "\n",
    "Define and train a neural network, called `digit_NN`, for the digit data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a470578c102e02becbf890437dd3ad46",
     "grade": true,
     "grade_id": "cell-a44e5c4855083891",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9938650306748467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load the already backpropagated weights for the network\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, test_size = 0.3,  random_state=11)\n",
    "\n",
    "# create network structure (taken from notebook form last week)\n",
    "digit_NN = NN(layer_sizes = [64, 65, 3])\n",
    "\n",
    "# train neural network\n",
    "costs = optimize(digit_NN, X_train, Y_train, alpha = 0.001, iterations = 1000, desc = 'iteration')\n",
    "\n",
    "def compute_accuracy(H, Y):\n",
    "    return ((np.argmax(H, axis = 1) == np.argmax(Y, axis = 1))*1).mean()\n",
    "\n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digit_NN.forward(X_test)\n",
    "\n",
    "# print(\"\\nThe network accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. Which configuration of layers gave you the best results?**\n",
    "\n",
    "*I took the configuration of network from last week. This gives the best results with an alpha of 0.001 and iterations of 1000. This is the best because the accuracy is 1.0. So i'll leave it at that.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab934625cd10c7b3204e679acf2e73d4",
     "grade": false,
     "grade_id": "cell-1eebb2263865eebb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### *Case 3: Iris*\n",
    "\n",
    "As a last use-case, let's look at data you have seen before in a different context. You have used the Iris dataset in an unsupervised learning context. But you can also use supervised learning to achieve something similar. The Iris samples are categorized into three classes. We can use a neural network to classify the data based on those classes. Load the Iris data below, read the description of the data set, and check what the features and labeled classes correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ecca19b3ee19e4d324800c403e12026",
     "grade": false,
     "grade_id": "cell-c6dd117f7ea1b59d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_raw = iris.data\n",
    "y_raw = iris.target\n",
    "\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "756dec2b5f11e1e60e58aaece7acad16",
     "grade": false,
     "grade_id": "cell-1099b7da8ee29ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6a \n",
    "\n",
    "The data should be modified so that it can be used with a neural network.\n",
    "\n",
    "First, you should normalize `X_raw`, i.e. make sure that all features are converted to values between 0 and 1. Store the result in a variable `X`.\n",
    "\n",
    "*Hint:* Make sure to normalize each feature separately. What *Numpy* function could you use to compute all normalization constants at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f9e89680b13b4eb6389d7923709de6",
     "grade": true,
     "grade_id": "cell-67f314cb7b1f84f5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# normalize each feature using np.linalg.norm\n",
    "X = X_raw / (np.linalg.norm(X_raw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b78a62c194e8601a23e84ceb17d032c3",
     "grade": false,
     "grade_id": "cell-a584c8607450c073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6b\n",
    "\n",
    "Second, the variable `y_raw` contains class data (values 0, 1, or 2). Transform this to one-hot encoding and store the result in a variable `Y`. \n",
    "\n",
    "*Hint:* Making good use of existing *Numpy* functions will save some time here. Specifically the [eye](https://numpy.org/doc/stable/reference/generated/numpy.eye.html) function and [index arrays](https://numpy.org/doc/stable/user/basics.indexing.html#index-arrays) might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f269b0acf588d9e4db23ca1b396ec3",
     "grade": true,
     "grade_id": "cell-b4f280c70a53f8d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# transform y_raw to one-hot-encoding\n",
    "n_values = np.max(y_raw) + 1\n",
    "Y = np.eye(n_values)[y_raw]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11969550da928466182d562209137229",
     "grade": false,
     "grade_id": "cell-71344bd658cc6fa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6c\n",
    "\n",
    "Now split the data in a train and test set. Make sure that the train size is 70% and the test size is 30% of the data. You may use library functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7126bb4d6909e43de917177482334a09",
     "grade": true,
     "grade_id": "cell-d618314b68bcf855",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# split the data in a train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, test_size = 0.3,  random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b638445980d6f4e0d4f994f5466b034",
     "grade": false,
     "grade_id": "cell-6c31f7982eae45c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6d\n",
    "\n",
    "Finally create a neural network, train it on the train data, and compute the accuracy with the test data. It should be possible to get an accuracy above 90%.\n",
    "\n",
    "You can use the function `plot_costs` to monitor the cost during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9b3cc3d94e4b803d56e303491147c9",
     "grade": true,
     "grade_id": "cell-e7bce2c764dc21ac",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqd0lEQVR4nO3deXxcdb3/8dcn+542S5e0adPSBcrS0qalRUAUoYAggqgFZFGkFxW9rj/9Xa/K/eF1uV696BVEZBcoKCBWLvsVKLUbaSld6L4mTdIm6ZKkadIm+f7+OCdlmmaZNpOczOT9fDzmkZmzfnLOzHvOfM9mzjlERCT6xQVdgIiIRIYCXUQkRijQRURihAJdRCRGKNBFRGKEAl1EJEYo0HvAzO40s8d7adq3mNnC3ph2yDyKzMyZWUJvzqe/MbMvmdluM6s3s9wwhu/1dREU8zxsZvvMbFmY4zxiZj/u7dp66kQ+n2b2ppl9sbdr6m0K9C74H/i2R6uZHQp5fUPQ9Q1EPf0SMrNE4FfAJc65DOdcTSSn39ci8GVzHnAxMNI5N6MXpi99SIHeBf8Dn+GcywB2AleGdHsi6PrkpAwFUoC1QRfST4wGtjvnDgZdiPScAr3nkszsMTOrM7O1Zlbc1sPMCszsWTOrMrNtZva1ziZiZrlmNt/Mav2fvqe063+qmb1mZnvNbIOZfcbvPtPMKs0sPmTYq81slf88zsy+Z2ZbzKzGzP5kZjmd1FDg17DXzDab2W0h/e40s2fM7Gn/f11hZpND+m83s++Y2SozO2hmD5rZUDN7yR/+dTMbHDL8TDNbZGb7zew9M7swpN+bZnaXmf3DH/dVM8vzey/w/+73fynN6uD/SDazu82s3H/c7XebAGwIGf/vHSyGTqdvZv/pN01sM7PLQrpn+/9vhZntMrMfh66PdrXFm9m/+OujzsyWm1mh3+9cM3vHzA74f88NGe8WM9vqj7PNzG4ws9OA+4BZfq37O5lnh+vVzG4FHggZ/9/ajdfV9Aeb2f/49Sw1s1NCxuvwvdpJbW/6y2uRP4+/+Z+FJ/zPwjtmVhQyfFfLaIyZveXX9BqQ125enb7nYoZzTo8wHsB24GPtut0JNAKXA/HAT4Elfr84YDnwQyAJGAtsBWZ3Mv2ngD8B6cAZwC5god8vHSgFPg8kAFOBauB0v/8W4OKQaf0Z+J7//OvAEmAkkAz8Hpjn9ysCHJDgv34LuBdvC3YKUAVcFPK/HgGuBRKBbwPbgMSQ5bMEbwt4BLAHWAGc7c/378CP/GFHADX+covD+8lfA+T7/d/0/6cJQKr/+mcd1dzJsvx/fi1DgHxgEXBXOON31B+4xf/fb/PX85eAcsD8/s/7yzXdn+cy4J86mf53gNXARMCAyUAukAPsA2701/F1/utcf7q1wER/GsND1v0t+O+TLpZHV+u1y/E76g88AuwFZvi1PgE8Fc57tYPpvwlsxtuAyQbeBzYCH/PHfwx42B+202Xk91+M15yWDFwA1AGPn8B77otB50yPcyroAqLlQeeB/nrI60nAIf/5OcDOdsP/37Y3Z7vu8X5gnBrS7Sd8EOifBd5uN87v+SAgfww85D/PBA4Co/3X69o+vP7r4f68EggJL6AQaAEyQ4b9KfBIyP+6JKRfHFABnB+yfG4I6f8s8LuQ118Fnveffxf4Y7v/5xXgZv/5m8C/hvT7MvCy//xozV2sqy3A5SGvZ+M1K3Q7fkf98UJtc8jrNH+YYXhfYE1Aakj/64A3Opn+BuCqDrrfCCxr122xP+90YD/wqdD5hNTWVSB3t167G/+4/niB/kDI68uB9eG8VzuY/pvA90Ne/xJ4KeT1lcDKMJbRKKAZSA/p9yQfBHo477moD/So2PHTz1WGPG8AUszboTYaKGj3MzUeeLuDaeTjhWppSLcdIc9HA+e0m1YC8Ef/+ZPAIjP7EnANsMI5tyNk3L+YWWvIuC14QRSqANjrnKtrV0NxyOuj9TnnWs2szB+vze6Q54c6eJ0RUtOnzezKkP6JwBshr9sv1wzCV8Cxy29HuzpPxtF6nHMNZoZfUw5e7RV+N/C+7ErbT8BXiPeF0177mvFfj3DOHTSzz+L9KnrQzP4BfMs5tz6MusNZryejs/XT3Xu1I+G+bzpdRn6/fe7YfQE78JZ3W13dveeingK995QC25xz48MYtgpv66IQaPuQjmo3rbeccxd3NLJz7n0z2wFcBlyPF/Ch437BOfeP9uOFtk3iNSHkmFlmyId/FF7TT5vCkHHj8Jpxyrv9745Xire1dFu3Qx4vnMuDluN9gNt2fI4i/DpP9PKjpXhb6HnOueYwhz8FWNOue1vNoUYBLwM4514BXjGzVLxfZH8Azg+j3nDWa1dOZnl0+l7toa6WUQVeu356SKiP4oP6e/KeixraKdp7lgG1ZvZdM0v1d4adYWbT2w/onGsBngPuNLM0M5sE3BwyyAvABDO70cwS/cd0f6dVmyeBr+G1Hf45pPt9wL+b2WgAM8s3s6s6qKEUr635p2aWYmZnAbfitY+2mWZm1/i/QL6OF2RLTnC5ADwOXGlms/3lkmJmF5rZyDDGrQJa8fZJdGYe8K/+/5qHtx8j3PMFwpn+Uc65CuBV4JdmlmXeTuhTzOzDnYzyAHCXmY03z1nmHQv/It46vt7MEvwt8knAC+btXP6EmaXjLfN6vF9Z4G3NjjSzpE7qC2e9dqXL6XcgnPfqyep0Gfm/SEuAfzOzJDM7D6+5pk1P3nNRQ4HeS/yQvhJvJ9Q2vB1DD+Dt+OnIHXg/LSvx2igfDplWHXAJMAdvK6US+Dnezp8284ALgb8756pDuv8amA+8amZ1eAF8Tic1XIfXhlwO/AWv3fO1kP5/xWsjbdsxdY1z7kgn0+qUHzJXAf+CF6CleDsLu30/OucagH8H/uEfrTCzg8F+jPfhXoW3A3KF3y2c2sKZfns34e34fh9v2TyDt6+iI7/C2/n9Kt6Ozgfx2sVrgCuAb+HtrPs/wBX+uozzu5fj7Yz8MN5+BfB2Nq8FKs0sdL2H6m69diWc6R8V5nv1pHSzjMD7dXoO3jL6Ed4O1bZxT/o9F03a9tKLdMnM7gTGOec+F3QtItKxmPp2EhEZyBToIiIxQk0uIiIxQlvoIiIxIrDj0PPy8lxRUVFQsxcRiUrLly+vds7ld9QvsEAvKiqipKQkqNmLiEQl/yTCDqnJRUQkRijQRURihAJdRCRGKNBFRGKEAl1EJEZ0G+hmVmhmb5jZOvNusfbPHQxjZvYb825vtcrMpvZOuSIi0plwDltsxruY/gozywSWm9lrzrn3Q4a5DBjvP84BfkfnV/QTEZFeEM7lSiuccyv853V4tzQb0W6wq4DHnGcJMMjMOrt8aI9sqKzjl69uoKa+qTcmLyIStU6oDd2/w83ZwNJ2vUZw7C23yjg+9DGzuWZWYmYlVVVVJ1iqZ/Oeev7775upOXj4pMYXEYlVYQe6mWXg3fj368652va9OxjluKt+Oefud84VO+eK8/M7PHO1W/F+xc0tuqiYiEiosALdzBLxwvwJ59xzHQxSRsj9Jjn5e012Kz7OK7mlVYEuIhIqnKNcDO82Weucc7/qZLD5wE3+0S4zgQP+vRYjLiHO+zHQosv+iogcI5yjXD6Ed//I1Wa20u/2L/h3pXfO3Yd389bLgc1AA/D5iFfqi2sL9NbW3pqFiEhU6jbQnXML6biNPHQYB3wlUkV15egWuvJcROQYUXemaJx5gd6sLXQRkWNEXaAnxHuBrjwXETlW1AW6ttBFRDoWdYH+QRu6jnIREQkVdYEer0AXEemQAl1EJEZEXaDrxCIRkY5FXaAn+hdzaTyinaIiIqHCOVO0XxmWnYIZ/Pzl9SzeUsOw7GSGZacyLCuF4dkpDM1KITc96egZpSIiA0XUBXpKYjzfvmQir66tZNGWavbUNR3Xnp4YbwzJTKEoL40xeekU5aYzNj+dMXkZFA5OJSE+6n6YiIh0y1xAbdHFxcWupKSkx9NpaXXU1DdRcaCRytpGKv2/FfsPsb2mga1V9dQ2Nh8dPiUxjtOGZ3HmiGzOKMjmrMJsJgzJ1Ba9iEQFM1vunCvuqF/UbaG3Fx9nDMlKYUhWCpM76O+cY1/DEbZVH2RrVT3rK+tYvesAzy4v47HFOwAYnJbIrFNymTU2l/PH51OUl963/4SISAREfaB3x8zISU8iJz2JaaMHH+3e2urYVnOQd3fuZ/GWGhZvqebF1ZUAnDosk0vPGMblZw5nwtDMoEoXETkhUd/kEinOOXbUNPC/6/fw8poKSnbswzmYUjiI62eM4orJw0lLivnvPxHp57pqclGgd2JPbSPz3yvnqXdK2bynnsyUBG6aNZovfGgMuRnJQZcnIgOUAr0HnHOU7NjHI//YzotrKkhJiOdzM0dxx0fGk52WGHR5IjLAxPRO0d5mZkwvymF6UQ6b99RxzxtbeHDhNp5ZXsY3L5nIddMLdRikiPQLSqITMG5IJv/12Sm88NXzmTgskx88v4ar713Ext11QZcmIqJAPxmTCrKYd9tMfnv92ZTvP8QVv1nI79/aQqsuGCYiAVKgnyQz44qzCnjlGxfwkVPz+elL67n10Xc40HAk6NJEZIBSoPdQXkYy931uGnd98gwWbq7mE/csZEOlmmBEpO8p0CPAzLhx5miemjuThsMtXHvfIpZt2xt0WSIywCjQI2ja6Bye/8qHyM9M5sYHl/La+7uDLklEBpBuA93MHjKzPWa2ppP+2Wb2NzN7z8zWmtnnI19m9BgxKJVnbj+XU4dlcvvjy3l1bWXQJYnIABHOFvojwKVd9P8K8L5zbjJwIfBLM0vqeWnRKyc9iSdum8mZI7K548l3eWtjVdAlicgA0G2gO+cWAF01CDsg08wMyPCHbe5i+AEhIzmBRz8/g3FDMpj7WAnLd6hNXUR6VyTa0H8LnAaUA6uBf3bOdXh/ODOba2YlZlZSVRX7W63ZaYn88dYZFAxKZe5jyynd2xB0SSISwyIR6LOBlUABMAX4rZlldTSgc+5+51yxc644Pz8/ArPu/3Izknnw5mKaWx1feOQdaht1nLqI9I5IBPrngeecZzOwDTg1AtONGWPzM/jdDVPZVn2Qbz69kqAuiCYisS0Sgb4TuAjAzIYCE4GtEZhuTDl3XB7f//hpvL5uDw8u3BZ0OSISg7q92qKZzcM7eiXPzMqAHwGJAM65+4C7gEfMbDVgwHedc9W9VnEUu+XcIpZsreFnL61n6ujBTB01uPuRRETCpOuh97EDh47w8d+8jXPwyjcuICNZVzAWkfB1dT10nSnax7JTE/n1nCmUHzjET15cF3Q5IhJDFOgBmDY6hy+eN4Ynl+7k7U2xf/imiPQNBXpAvnXJRMbmp/PdZ1ZRp0MZRSQCFOgBSUmM5xfXTqaitpFfv74p6HJEJAYo0AM0bfRg5kwv5OFF23UNdRHpMQV6wL4z+1QyUxL44V/X6IQjEekRBXrActKT+M7siSzdtpf575UHXY6IRDEFej8wZ/oozhiRxX+8vIHGIy1BlyMiUUqB3g/Exxn/ctlp7Np/iMcWbw+6HBGJUgr0fuLccXl8eEI+97yxhQMNOoxRRE6cAr0f+d5lp1LbeIR739wcdCkiEoUU6P3IacOzuPrsETy8aDuVBxqDLkdEoowCvZ/5xscm0NrquO+tLUGXIiJRRoHezxTmpHHN1BHMW7aTPbXaSheR8CnQ+6GvfGQcza2O3y/QfUJEJHwK9H5odG46V00p4ImlO6iqawq6HBGJEgr0fuqOj4zjcHMrf3hbW+kiEh4Fej81Nj+DK84q4IklOzhwSMeli0j3FOj92NwLxnLwcAtPLdsZdCkiEgUU6P3YGSOyOfeUXB7+x3YON7cGXY6I9HMK9H7utgvGUlnbyAurdCVGEemaAr2fu3BCPuOHZHD/gq26XrqIdEmB3s+ZGbddMJb1lXX8Y3NN0OWISD/WbaCb2UNmtsfM1nQxzIVmttLM1prZW5EtUa6aUkB+ZjIPLNQhjCLSuXC20B8BLu2sp5kNAu4FPuGcOx34dEQqk6OSE+K5fsYo3tpYxY6ag0GXIyL9VLeB7pxbAOztYpDrgeecczv94fdEqDYJcf05o4g34/ElO4IuRUT6qUi0oU8ABpvZm2a23Mxu6mxAM5trZiVmVlJVVRWBWQ8cQ7NSmH3GMJ5+p5RDh3WbOhE5XiQCPQGYBnwcmA38wMwmdDSgc+5+51yxc644Pz8/ArMeWG6eVURtYzN/Xbkr6FJEpB+KRKCXAS875w4656qBBcDkCExX2pleNJhTh2Xy6OIdOoRRRI4TiUD/K3C+mSWYWRpwDrAuAtOVdsyMm2YVsa6iluU79gVdjoj0M+EctjgPWAxMNLMyM7vVzG43s9sBnHPrgJeBVcAy4AHnXKeHOErPfPLsAjJTEnhk0fagSxGRfiahuwGcc9eFMcwvgF9EpCLpUlpSAtdOG8njS3ZQU99EbkZy0CWJSD+hM0Wj0PUzRnGkxfHsirKgSxGRfkSBHoXGD82kePRgnlpWqp2jInKUAj1KzZkxiq3VB1m6ratzvkRkIFGgR6mPnzmczJQE5unmFyLiU6BHqdSkeK4+ewQvralk38HDQZcjIv2AAj2KzZk+isPNrTz3rs4cFREFelSbVJDF5MJBPLVsp3aOiogCPdpdP6OQTXvqWbFTZ46KDHQK9Ch3xVkFpCfF8+TS0qBLEZGAKdCjXHpyAledPYL/WV3OgUNHgi5HRAKkQI8B100fReORVubrsroiA5oCPQacOTKb0wuymKczR0UGNAV6jJgzYxTvV9SyeteBoEsRkYAo0GPEVVMKSEmMY94y7RwVGagU6DEiKyWRj59ZwPyVuzjY1Bx0OSISAAV6DLluRiEHD7fwwqryoEsRkQAo0GPItNGDGTckQ80uIgOUAj2GmBlzpheysnQ/6ytrgy5HRPqYAj3GXDN1JEnxcTylrXSRAUeBHmNy0pOYfcYw/vLuLhqPtARdjoj0IQV6DLpueiEHDh3h5TWVQZciIn1IgR6DZo7NZXRumu5mJDLAKNBjUFyc8dnphSzdtpetVfVBlyMifaTbQDezh8xsj5mt6Wa46WbWYmbXRq48OVnXTh1JfJzx9DvaOSoyUISzhf4IcGlXA5hZPPBz4JUI1CQRMCQrhYtOHcIzy8s43NwadDki0ge6DXTn3AJgbzeDfRV4FtgTiaIkMq6bMYqag4d5fd3uoEsRkT7Q4zZ0MxsBXA3cF8awc82sxMxKqqqqejpr6cYFE/IpyE7RzlGRASISO0XvBr7rnOv2oGfn3P3OuWLnXHF+fn4EZi1diY8zPl1cyMLN1ZTubQi6HBHpZZEI9GLgKTPbDlwL3Gtmn4zAdCUCPjO9EIA/l2jnqEis63GgO+fGOOeKnHNFwDPAl51zz/d0uhIZIwal8uEJ+fyppIzmFu0cFYll4Ry2OA9YDEw0szIzu9XMbjez23u/PImEOdNHUVnbyFsbtd9CJJYldDeAc+66cCfmnLulR9VIr7jotCHkZSQzb1kpF502NOhyRKSX6EzRASAxPo5rp43kjQ172F3bGHQ5ItJLFOgDxHUzCml1jieX6hBGkVilQB8gRuem85GJQ3hi6U6dOSoSoxToA8jN5xZRXd/Ei6srgi5FRHqBAn0AOX9cHmPz03l40fagSxGRXqBAH0Di4oybZxXxXul+3t25L+hyRCTCFOgDzKemjSQjOYFHtZUuEnMU6ANMRnIC104byf+srmBPnQ5hFIklCvQB6OZzizjSokMYRWKNAn0AGpOXzkcm5vP4kh00Hun2IpkiEiUU6APUbReMpbr+MM+uKAu6FBGJEAX6ADVrbC6TR2bzhwVbaWl1QZcjIhGgQB+gzIx/+vApbK9p4JW1lUGXIyIRoEAfwGafPoyi3DTue2sLzmkrXSTaKdAHsPg447YLxrKq7ACLt9YEXY6I9JACfYD71NSR5GUkcd9bW4MuRUR6SIE+wKUkxvOF88awYGMVK0v3B12OiPSAAl24aVYRg9MSufv1jUGXIiI9oEAXMpITuO2Csby5oYoVumiXSNRSoAsAN88qIic9ibtf3xR0KSJykhToAkB6cgJzLxjLgo1VLN+hrXSRaKRAl6NumjWa3PQk/us1taWLRCMFuhyVlpTAly48hYWbq1mwsSrockTkBHUb6Gb2kJntMbM1nfS/wcxW+Y9FZjY58mVKX7lx1mgKc1L5yYvrdI0XkSgTzhb6I8ClXfTfBnzYOXcWcBdwfwTqkoAkJ8Tzndmnsr6yjud0JUaRqNJtoDvnFgB7u+i/yDnXthdtCTAyQrVJQK48aziTCwfxy1c3cuiwrpcuEi0i3YZ+K/BSZz3NbK6ZlZhZSVWV2mj7KzPj+5efRmVtIw8u1CUBRKJFxALdzD6CF+jf7WwY59z9zrli51xxfn5+pGYtvWDGmBxmnz6Ue97Ywq79h4IuR0TCEJFAN7OzgAeAq5xzumxfjPjBFZNwOO762/tBlyIiYehxoJvZKOA54EbnnA5gjiEjB6fx1Y+O5+W1lby5YU/Q5YhIN8I5bHEesBiYaGZlZnarmd1uZrf7g/wQyAXuNbOVZlbSi/VKH/vi+WMYm5fOj+av1Q2lRfo5C+pONcXFxa6kRNkfDRZuquZzDy7laxeN55sXTwi6HJEBzcyWO+eKO+qnM0WlW+eNz+Pqs0dw7xubWVt+IOhyRKQTCnQJy4+unMTg9CS+/edVHG5uDbocEemAAl3CMigtiZ9cfSbrKmq5543NQZcjIh1QoEvYLp40lE9OKeCeNzazZpeaXkT6GwW6nJA7P3E6eRnJfHXeu9Q3NQddjoiEUKDLCRmUlsTdc6awo+YgP3y+wwtwikhAFOhywmaOzeVrF43nuXd38exyXZFRpL9QoMtJ+epHx3POmBx+8Nc1bNxdF3Q5IoICXU5SfJzx6zlnk5aUwG2PlbC/4XDQJYkMeAp0OWnDslP4/Y1TKd9/iK/Oe5fmFh2fLhIkBbr0yLTROfz4k2fw9qZqfvbS+qDLERnQEoIuQKLfZ6eP4v3yWh5YuI3RuWncOKso6JJEBiQFukTED66YRNm+Q/xw/lryM5O59IzhQZckMuCoyUUiIiE+jt9eP5UphYP42lMrWbat09vQikgvUaBLxKQmxfPQzdMZOTiVWx99h1Vl+4MuSWRAUaBLRA1OT+KPt55Ddmoin3tgKavLdM0Xkb6iQJeIGzEolXm3zSQzJZHPPbhUF/IS6SMKdOkVhTlpPDV3JhnJCdzwwFKW79gXdEkiMU+BLr2mLdQHpSVywwNLeGO9bjQt0psU6NKrCnPSeOb2czklP4MvPlaii3mJ9CIFuvS6/Mxknpo7k3PG5PCtP7/H3a9vpLU1mJuTi8QyBbr0icyURB7+/HSumTqCu1/fxFeeXMFB3SBDJKIU6NJnkhPi+eWnJ/OvHz+NV9ZW8qnfLaJ0b0PQZYnEjG4D3cweMrM9Ztbh7WnM8xsz22xmq8xsauTLlFhhZnzx/LE8dMt0du0/xMd/8zYvr6kMuiyRmBDOFvojwKVd9L8MGO8/5gK/63lZEusunDiEv91xHqNz07n98eX88K9raDzSEnRZIlGt20B3zi0Aurowx1XAY86zBBhkZroyk3SrKC+dZ790LreeN4bHFu/gmnsXsaFSdz8SOVmRaEMfAZSGvC7zux3HzOaaWYmZlVRVVUVg1hLtkhLi+MEVk3jw5mIqaxu58r8Xcs8bm3WzDJGTEIlAtw66dXhMmnPufudcsXOuOD8/PwKzllhx0WlDee0bF3Dx6UP5xSsbuPreRayvrA26LJGoEolALwMKQ16PBMojMF0ZYHIzkrnn+qn87oapVBw4xBW/WchPXlxHvQ5vFAlLJAJ9PnCTf7TLTOCAc64iAtOVAeqyM4fz6jc+zLXTRvKHt7fy0f98k+ff3YVzOhlJpCvhHLY4D1gMTDSzMjO71cxuN7Pb/UFeBLYCm4E/AF/utWplwMhJT+JnnzqLv3z5QwzLTuHrT6/kM79fzIqdusiXSGcsqK2e4uJiV1JSEsi8Jbq0tDqefqeUX722ger6w8w+fSjfmX0q44ZkBF2aSJ8zs+XOueIO+ynQJVocbGrmgbe3cf+CLRw60sKnpxVyx0fHUZiTFnRpIn1GgS4xpaa+id++sZnHl+yg1cEnp4zgSxeeoi12GRAU6BKTKg4c4v4FW5m3bCdNza1cdsYwvnzhOM4YkR10aSK9RoEuMa26vomHFm7jscU7qG9qZkZRDrd8qIhLJg0lIV7Xn5PYokCXAeHAoSP86Z1SHl28nbJ9hyjITuHGWUXMmV7I4PSkoMsTiQgFugwoLa2O/123m0cWbWfRlhqS4uO45PShfKa4kPPG5REX19HJzSLRoatAT+jrYkR6W3ycccnpw7jk9GFsqKxj3rKdPL9yFy+sqmDEoFQ+NW0kn542UkfHSMzRFroMCE3NLbz2/m6efqeUhZurcQ6KRw/myskFXH7mcPIzk4MuUSQsanIRCbFr/yH+sqKMF1ZVsL6yjjiDWafkcuVZBVx6xjAGpam9XfovBbpIJzburuOF98qZ/14522saSIgzZozJ4WOnDeXiSUPVLCP9jgJdpBvOOdaW1/Li6gpeX7ebjbvrATh1WCYfO20oH5s0lLNGZGuHqgROgS5ygnbUHOS193fz+rrdvLN9Hy2tjsFpiXxoXB4XjM/nvPF5FAxKDbpMGYAU6CI9sL/hMG9uqGLBpioWbqpmT10TAKfkp3P++HzOH5/H9DE5ZKUkBlypDAQKdJEIcc6xcXc9b2+q4u1N1SzdVkPjkVbiDCYVZDG9KIdzxuQwvSiH3AwdOSORp0AX6SWNR1pYsWMfS7ftZdm2vazYuY+mZu9+qOOGZDC9KIcZYwYzpXAwRblpmKkNXnpGgS7SRw43t7J6136WbdvHsm01lGzfR51/C71BaYlMHjmIyYWDOLvQ+5ujSxLICVKgiwSkpdWxcXcdK0v3817pflaW7mfj7jpa/Y/d6Nw0Jo8cxJkjsplUkMWk4Vm67ox0SYEu0o8cbGpm9a4DrCzdz8qd+3mvbD8VBxqP9i/ITvHCvSCbScOzOL0gi5GDU9VcI4Cu5SLSr6QnJzBzbC4zx+Ye7VZT38S6ijrerzjA2vJa3i+v5e/r9xzdks9MSeC0YVmMH5rB+CEZTBiayfihmeRlJCno5SgFukg/kJuRzHnjkzlvfN7RbocOt7Bhdx1ry72Q31hZx9/eK6e2sfnoMIPSEpkwJPOYoB83NIP8jGQF/QCkQBfpp1KT4plSOIgphYOOdnPOUVXXxMbd9WzaU+f93X180GckJzAmL50xeekU5aUzNuR5dqqOl49VCnSRKGJmDMlKYUhWyjFb86FBv3lPHdtrGthafZB3S/fxwqryo003ALnpSceEfVFuOoU5qYzKSSM7NVFb9lFMgS4SAzoLevAuHVy6t4GtVQfZVn2Q7TUH2Vp1kLc2VvHn5WXHDJuZnMDInDQKB6dSmJPGqJw0CnNSKRycxsjBaaQmxfflvyUnKKxAN7NLgV8D8cADzrmfteufDTwOjPKn+Z/OuYcjXKuInITkhHjGDclk3JDM4/rVNzWzs6aB0n0NlO5toGzfIXbubWBb9UEWbKqi8UjrMcPnZyYfDfvh2amMGJTC8OxUCgalUjAoRVv4Aes20M0sHrgHuBgoA94xs/nOufdDBvsK8L5z7kozywc2mNkTzrnDvVK1iERERnKCf4hk1nH9nHNU1x9m594GyvzAL917iNJ9DazYuY/KAxUcaTn2sOfUxHgKBqV4AZ+dyvD2z7NTtZXfi8LZQp8BbHbObQUws6eAq4DQQHdApnlfzRnAXqC5/YREJHqYGfmZyeRnJjNt9ODj+re2Oqrrmyg/0Ej5/kP+o5GKA4coP9DI+so9VPkXMgs1OC2RYdmpDM1KZmhmCkOzUz54npXC0OxkctOTidelik9YOIE+AigNeV0GnNNumN8C84FyIBP4rHOutd0wmNlcYC7AqFGjTqZeEekn4uI+aLcPPRInVFNzC7sPNFF+wAv8igON7Np/iD21jeyubWJteS3V9U20P78xPs7Iz0j2gj7LD/qsZIZkpTAs5LWaeI4VTqB3tLTan146G1gJfBQ4BXjNzN52ztUeM5Jz9wP3g3em6AlXKyJRJTkhnlG5aYzK7fzOT80trVTXH2Z3bSOVtY1Hw77t9Y6aBpZt38v+hiPHjZuUEEd+RvLRXxJ5Ic+97knkZ6SQn5k8IJp6wgn0MqAw5PVIvC3xUJ8Hfua86whsNrNtwKnAsohUKSIxKyE+jmHZKQzLTmFyF8M1HmlhT20Tu+sa2R0S+tV1TVTVN1G6t4F3d+6j5uDh47b4wdtf4IV+Ukjgt/sSyPSae5IS4nrt/+1N4QT6O8B4MxsD7ALmANe3G2YncBHwtpkNBSYCWyNZqIgMbCmJ3W/tg7fFv/fgYfbUNVFd30SVH/hVdd6jur6JDZV1LKyrPuZkrFCD0hKPCfy8jGTyMpO88M9IJjfDe56bkURyQv/Z8u820J1zzWZ2B/AK3mGLDznn1prZ7X7/+4C7gEfMbDVeE813nXPVvVi3iEiHEuLjjrbtd6fxSAvV9U1U1x8+GvjeF0Aj1XWHqapvYmXpfqrrm2g43NLhNLJSEsjLTCYv/YPQb3u0BX++/4WQltS7p/7oaosiImFoONx8NORr/C8B78ugiZp6r3t1fRPVdU2dbvmnJsaTl5nEzbOK+OL5Y0+qDl1tUUSkh9KSEhiVm9Btkw94R/fsPXiY6jov9Kv80G/7AsjP7J3bEyrQRUQiLDkhnuHZqQzPTu3T+UbnrlwRETmOAl1EJEYo0EVEYoQCXUQkRijQRURihAJdRCRGKNBFRGKEAl1EJEYEduq/mVUBO05y9DygP14rpr/WBf23NtV1YlTXiYnFukY75/I76hFYoPeEmZV0di2DIPXXuqD/1qa6TozqOjEDrS41uYiIxAgFuohIjIjWQL8/6AI60V/rgv5bm+o6MarrxAyouqKyDV1ERI4XrVvoIiLSjgJdRCRGRF2gm9mlZrbBzDab2fd6eV6FZvaGma0zs7Vm9s9+9zvNbJeZrfQfl4eM83/92jaY2eyQ7tPMbLXf7zdmZhGob7s/zZVmVuJ3yzGz18xsk/93cF/WZmYTQ5bLSjOrNbOvB7HMzOwhM9tjZmtCukVs+ZhZspk97XdfamZFPajrF2a23sxWmdlfzGyQ373IzA6FLLf7+riuiK23CNf1dEhN281sZQDLq7N8CO495pyLmgfeTaq3AGOBJOA9YFIvzm84MNV/nglsBCYBdwLf7mD4SX5NycAYv9Z4v98yYBbeTbRfAi6LQH3bgbx23f4D+J7//HvAz4OoLWR9VQKjg1hmwAXAVGBNbywf4MvAff7zOcDTPajrEiDBf/7zkLqKQodrN52+qCti6y2SdbXr/0vghwEsr87yIbD3WLRtoc8ANjvntjrnDgNPAVf11syccxXOuRX+8zpgHTCii1GuAp5yzjU557YBm4EZZjYcyHLOLXbemnkM+GQvlX0V8Kj//NGQ+QRR20XAFudcV2cE91pdzrkFwN4O5hep5RM6rWeAi8L5FdFRXc65V51zbXcWXgKM7GoafVVXFwJdXm388T8DzOtqGr1UV2f5ENh7LNoCfQRQGvK6jK4DNmL8nzpnA0v9Tnf4P48fCvlJ1Vl9I/zn7bv3lANeNbPlZjbX7zbUOVcB3hsOGBJQbeBtUYR+0PrDMovk8jk6jh/GB4DcCNT4BbyttDZjzOxdM3vLzM4PmXdf1RWp9dYby+t8YLdzblNItz5fXu3yIbD3WLQFekffTL1+3KWZZQDPAl93ztUCvwNOAaYAFXg/+bqqr7fq/pBzbipwGfAVM7ugi2H7tDYzSwI+AfzZ79RflllnTqaOiNdoZt8HmoEn/E4VwCjn3NnAN4EnzSyrD+uK5HrrjXV6HcduNPT58uogHzodtJP5RKy2aAv0MqAw5PVIoLw3Z2hmiXgr6wnn3HMAzrndzrkW51wr8Ae8pqCu6ivj2J/QEanbOVfu/90D/MWvY7f/E67tZ+aeIGrD+5JZ4Zzb7dfYL5YZkV0+R8cxswQgm/CbLI5jZjcDVwA3+D+98X+e1/jPl+O1u07oq7oivN4ivbwSgGuAp0Pq7dPl1VE+EOB7LNoC/R1gvJmN8bcA5wDze2tmflvVg8A659yvQroPDxnsaqBt7/t8YI6/Z3oMMB5Y5v/sqjOzmf40bwL+2sPa0s0ss+053k61NX4NN/uD3Rwynz6rzXfMllN/WGYh84vU8gmd1rXA39uC+ESZ2aXAd4FPOOcaQrrnm1m8/3ysX9fWPqwrkustYnX5Pgasd84dba7oy+XVWT4Q5Husqz2m/fEBXI63N3kL8P1entd5eD9vVgEr/cflwB+B1X73+cDwkHG+79e2gZCjMoBivA/DFuC3+Gfp9qC2sXh7zN8D1rYtC7z2tf8FNvl/cwKoLQ2oAbJDuvX5MsP7QqkAjuBt6dwayeUDpOA1KW3GO0phbA/q2ozXVtr2Pms7suFT/vp9D1gBXNnHdUVsvUWyLr/7I8Dt7Ybty+XVWT4E9h7Tqf8iIjEi2ppcRESkEwp0EZEYoUAXEYkRCnQRkRihQBcRiREKdBGRGKFAFxGJEf8fHnJaLBhadVgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# create network structure (taken from notebook form last week)\n",
    "digit_NN = NN(layer_sizes = [4, 6, 3])\n",
    "\n",
    "# train neural network\n",
    "costs = optimize(digit_NN, X_train, Y_train, alpha = 0.002, iterations = 20000, desc = 'iteration')\n",
    "\n",
    "# plot costs: enables the consideration elbow of curve\n",
    "plot = plot_costs(costs)\n",
    "\n",
    "def compute_accuracy(H, Y):\n",
    "    return ((np.argmax(H, axis = 1) == np.argmax(Y, axis = 1))*1).mean()\n",
    "\n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digit_NN.forward(X_test)\n",
    "\n",
    "# print(\"\\nThe network accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9b6470150da8fc737b9d3fcb5f5a6fe",
     "grade": false,
     "grade_id": "cell-841f78266d86af85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Final thoughts\n",
    "\n",
    "Done! Let's have a final look at what you did. You've implemented `LogisticLayer` and `NN`. You essentially created a modular way for composing neural networks. Next module you will even expand more on this modularity.\n",
    "\n",
    "We gave you the equations for working out the gradients. And you might be wondering where they came from. One of the subtle points that you might not have noticed is that this modular computation of the gradients is done by relying heavily on the application of the chain rule. Let's consider this with a 2 layer network using the non-vectorized maths ($w_1$ and $b_1$ are just scalar parameters). Then, the network is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1 = x &&\n",
    "z_1 = w_1a_1+b_1 &&\n",
    "a_2 = g(z_1) &&\n",
    "z_2 = w_2a_2+b_2 &&\n",
    "a_3 = g(z_2) &&\n",
    "\\hat{y} = a_3\n",
    "\\end{align} \\\\\n",
    "l = -y\\ln(\\hat{y}) - (1 - y)\\ln(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "The loss of this network $l$ is the result of applying quite a few equations in order. Each of these equations can be seen as a function, and so, in order to compute $\\frac{\\partial l}{\\partial w_1}$, we can use many applications of the chain rule to get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\frac{\\partial l}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "In principle you could compute this whole derivative as one single expression, but it is already impractical and you can imagine how unwieldy this would get if you increase the number of layers. What we did is break it up. During the backpropagation every logistic layer computes the term $\\frac{\\partial l}{\\partial a}$ for the previous layer. This no longer has to be worked out as part of the expression. So, starting with the last layer, the partial derivative w.r.t. the activation $a_3$ can be computed and the expression simplified: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial a_3}}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_3}}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "Then we can use the computed $\\frac{\\partial l}{\\partial a_3}$ and the activations at $a_2$ to further compute partial derivative w.r.t. $a_2$, resulting in:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_2}}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1} = \\boxed{\\frac{\\partial l}{\\partial a_2}}\\frac{\\partial a_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "The newly computed $\\frac{\\partial l}{\\partial a_2}$ can then be used directly to compute the partial derivative w.r.t. $w_1$. The computation of the term $\\frac{\\partial l}{\\partial a}$ at every layer is thus at the core of the backpropagation algorithm.\n",
    "\n",
    "Returning to the full vectorized equations from the assignment, the equations that you got for computing $\\frac{\\partial l}{\\partial W}$ were:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial W} = D^T \\cdot A &&\n",
    "D = A_{\\mathrm{next}}\\odot(1 \\ominus A_{\\mathrm{next}}) \\odot \\boxed{\\frac{\\partial l}{\\partial A_{\\mathrm{next}}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "are a vectorized version of working out:\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w} = \\boxed{\\frac{\\partial l}{\\partial a_\\mathrm{next}}}\\frac{\\partial a_\\mathrm{next}}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "So you've essentially been applying the chain rule at every step of the backpropagation algorithm.\n",
    "\n",
    "You now understand the basis of any neural network. From here on you will learn tricks to speed up the learning, avoid local minimums, avoid overfitting, and other improvements, but you now know the fundaments on which those are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
